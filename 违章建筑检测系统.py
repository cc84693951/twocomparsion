#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¿ç« å»ºç­‘æ£€æµ‹ç³»ç»Ÿ
åŸºäºè½®å»“æ£€æµ‹å’ŒåŒºåŸŸåƒç´ å·®æ¯”è¾ƒçš„ä¸¤æœŸå›¾åƒè¿ç« å»ºç­‘è¯†åˆ«
"""

import cv2
import numpy as np
import os
import json
from datetime import datetime
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.patches import Rectangle
import pandas as pd


class IllegalBuildingDetector:
    """è¿ç« å»ºç­‘æ£€æµ‹ç³»ç»Ÿ"""
    
    def __init__(self, output_dir="illegal_building_results"):
        """
        åˆå§‹åŒ–è¿ç« å»ºç­‘æ£€æµ‹ç³»ç»Ÿ
        
        Args:
            output_dir (str): è¾“å‡ºç›®å½•
        """
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        # ç³»ç»Ÿå‚æ•°
        self.min_building_size = (50, 50)  # æœ€å°å»ºç­‘ç‰©å°ºå¯¸ (50x50px/1080p)
        self.pixel_diff_threshold = 50     # åƒç´ å·®å¼‚é˜ˆå€¼
        self.contour_area_threshold = 2500  # è½®å»“é¢ç§¯é˜ˆå€¼ (50x50)
        self.illumination_range = (100, 1200)  # å…‰ç…§èŒƒå›´ (lux)
        self.adaptive_threshold = True     # æ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼
        
        print(f"è¿ç« å»ºç­‘æ£€æµ‹ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        print(f"è¾“å‡ºç›®å½•: {output_dir}")
        print(f"ç³»ç»Ÿå‚æ•°:")
        print(f"  æœ€å°å»ºç­‘ç‰©å°ºå¯¸: {self.min_building_size}")
        print(f"  åƒç´ å·®å¼‚é˜ˆå€¼: {self.pixel_diff_threshold}")
        print(f"  è½®å»“é¢ç§¯é˜ˆå€¼: {self.contour_area_threshold}")
    
    def load_image_with_chinese_path(self, path):
        """
        åŠ è½½åŒ…å«ä¸­æ–‡è·¯å¾„çš„å›¾åƒ
        
        Args:
            path (str): å›¾åƒè·¯å¾„
            
        Returns:
            ndarray: å›¾åƒæ•°æ®
        """
        try:
            img_array = np.fromfile(path, dtype=np.uint8)
            img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)
            if img is None:
                raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {path}")
            return img
        except Exception as e:
            print(f"è¯»å–å›¾åƒå¤±è´¥ {path}: {str(e)}")
            return None
    
    def perspective_correction(self, img1, img2):
        """
        3.2.3.1 é€è§†å˜æ¢çŸ©é˜µè¿›è¡Œè§’åº¦åå·®æ ¡æ­£
        ä»¥ç¬¬ä¸€æœŸå›¾åƒä½œä¸ºåŸºå‡†ï¼ˆæ ‡å®šå›¾ï¼‰ï¼Œå°†ç¬¬äºŒæœŸå›¾åƒé…å‡†åˆ°ç¬¬ä¸€æœŸå›¾åƒåæ ‡ç³»
        
        Args:
            img1 (ndarray): ç¬¬ä¸€æœŸå›¾åƒï¼ˆåŸºå‡†å›¾åƒï¼‰
            img2 (ndarray): ç¬¬äºŒæœŸå›¾åƒï¼ˆå¾…é…å‡†å›¾åƒï¼‰
            
        Returns:
            tuple: (åŸå§‹img1, æ ¡æ­£åçš„img2, å˜æ¢çŸ©é˜µ, ç‰¹å¾ç‚¹ä¿¡æ¯)
        """
        print("æ‰§è¡Œé€è§†å˜æ¢çŸ©é˜µè§’åº¦åå·®æ ¡æ­£...")
        
        # è½¬æ¢ä¸ºç°åº¦å›¾
        gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
        gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
        
        # ä½¿ç”¨SIFTæ£€æµ‹å™¨æ‰¾åˆ°ç‰¹å¾ç‚¹
        sift = cv2.SIFT_create(nfeatures=1000)
        
        # æ£€æµ‹å…³é”®ç‚¹å’Œæè¿°ç¬¦
        kp1, des1 = sift.detectAndCompute(gray1, None)
        kp2, des2 = sift.detectAndCompute(gray2, None)
        
        print(f"  ç¬¬ä¸€æœŸå›¾åƒæ£€æµ‹åˆ° {len(kp1)} ä¸ªç‰¹å¾ç‚¹")
        print(f"  ç¬¬äºŒæœŸå›¾åƒæ£€æµ‹åˆ° {len(kp2)} ä¸ªç‰¹å¾ç‚¹")
        
        if des1 is None or des2 is None or len(kp1) < 4 or len(kp2) < 4:
            print("  ç‰¹å¾ç‚¹ä¸è¶³ï¼Œè¿”å›åŸå›¾åƒ")
            return img1, img2, None, {"keypoints1": 0, "keypoints2": 0, "matches": 0}
        
        # ä½¿ç”¨FLANNåŒ¹é…å™¨è¿›è¡Œç‰¹å¾åŒ¹é…
        FLANN_INDEX_KDTREE = 1
        index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
        search_params = dict(checks=50)
        flann = cv2.FlannBasedMatcher(index_params, search_params)
        
        matches = flann.knnMatch(des1, des2, k=2)
        
        # åº”ç”¨æ¯”å€¼æµ‹è¯•ç­›é€‰å¥½çš„åŒ¹é…
        good_matches = []
        for match_pair in matches:
            if len(match_pair) == 2:
                m, n = match_pair
                if m.distance < 0.7 * n.distance:
                    good_matches.append(m)
        
        print(f"  ç­›é€‰åçš„è‰¯å¥½åŒ¹é…ç‚¹: {len(good_matches)}")
        
        if len(good_matches) < 4:
            print("  è‰¯å¥½åŒ¹é…ç‚¹ä¸è¶³4ä¸ªï¼Œè¿”å›åŸå›¾åƒ")
            return img1, img2, None, {"keypoints1": len(kp1), "keypoints2": len(kp2), "matches": len(good_matches)}
        
        # æå–åŒ¹é…ç‚¹çš„åæ ‡ï¼ˆäº¤æ¢srcå’Œdstï¼Œè®©ç¬¬äºŒæœŸå›¾åƒé…å‡†åˆ°ç¬¬ä¸€æœŸå›¾åƒï¼‰
        src_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)  # ç¬¬äºŒæœŸå›¾åƒçš„ç‚¹
        dst_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)  # ç¬¬ä¸€æœŸå›¾åƒçš„ç‚¹
        
        # ä½¿ç”¨RANSACè®¡ç®—é€è§†å˜æ¢çŸ©é˜µï¼ˆç¬¬äºŒæœŸâ†’ç¬¬ä¸€æœŸï¼‰
        M, mask = cv2.findHomography(src_pts, dst_pts, 
                                   cv2.RANSAC, 
                                   ransacReprojThreshold=5.0)
        
        if M is None:
            print("  æ— æ³•è®¡ç®—é€è§†å˜æ¢çŸ©é˜µï¼Œè¿”å›åŸå›¾åƒ")
            return img1, img2, None, {"keypoints1": len(kp1), "keypoints2": len(kp2), "matches": len(good_matches)}
        
        # è®¡ç®—å†…ç‚¹æ¯”ä¾‹
        inlier_ratio = np.sum(mask) / len(mask) if mask is not None else 0
        print(f"  é€è§†å˜æ¢å†…ç‚¹æ¯”ä¾‹: {inlier_ratio:.2%}")
        
        # åº”ç”¨é€è§†å˜æ¢æ ¡æ­£ç¬¬äºŒæœŸå›¾åƒåˆ°ç¬¬ä¸€æœŸå›¾åƒåæ ‡ç³»
        h, w = img1.shape[:2]  # ä½¿ç”¨ç¬¬ä¸€æœŸå›¾åƒå°ºå¯¸ä½œä¸ºåŸºå‡†
        corrected_img2 = cv2.warpPerspective(img2, M, (w, h))
        
        transform_info = {
            "keypoints1": len(kp1),
            "keypoints2": len(kp2),
            "matches": len(good_matches),
            "inlier_ratio": float(inlier_ratio),
            "transform_matrix": M.tolist() if M is not None else None
        }
        
        print("  é€è§†å˜æ¢æ ¡æ­£å®Œæˆ")
        return img1, corrected_img2, M, transform_info
    
    def analyze_image_lighting(self, img):
        """
        åˆ†æå›¾åƒå…‰ç…§ç‰¹æ€§
        
        Args:
            img (ndarray): è¾“å…¥å›¾åƒ
            
        Returns:
            dict: å…‰ç…§åˆ†æç»“æœ
        """
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        
        # è®¡ç®—å…‰ç…§ç»Ÿè®¡ä¿¡æ¯
        mean_brightness = np.mean(gray)
        std_brightness = np.std(gray)
        min_brightness = np.min(gray)
        max_brightness = np.max(gray)
        
        # è®¡ç®—åŠ¨æ€èŒƒå›´
        dynamic_range = max_brightness - min_brightness
        
        # è®¡ç®—ç›´æ–¹å›¾
        hist = cv2.calcHist([gray], [0], None, [256], [0, 256])
        
        # åˆ¤æ–­å…‰ç…§ç±»å‹
        if mean_brightness < 80:
            lighting_type = "dark"  # æš—å…‰
        elif mean_brightness > 180:
            lighting_type = "bright"  # äº®å…‰
        else:
            lighting_type = "normal"  # æ­£å¸¸å…‰ç…§
            
        # åˆ¤æ–­å¯¹æ¯”åº¦
        if std_brightness < 30:
            contrast_type = "low"  # ä½å¯¹æ¯”åº¦
        elif std_brightness > 80:
            contrast_type = "high"  # é«˜å¯¹æ¯”åº¦
        else:
            contrast_type = "normal"  # æ­£å¸¸å¯¹æ¯”åº¦
        
        return {
            'mean_brightness': float(mean_brightness),
            'std_brightness': float(std_brightness),
            'min_brightness': int(min_brightness),
            'max_brightness': int(max_brightness),
            'dynamic_range': int(dynamic_range),
            'lighting_type': lighting_type,
            'contrast_type': contrast_type,
            'histogram': hist
        }
    
    def adaptive_normalize_image(self, img, lighting_analysis=None, conservative_mode=False):
        """
        è‡ªé€‚åº”å½’ä¸€åŒ–å›¾åƒå¤„ç†
        æ ¹æ®å›¾åƒå…‰ç…§ç‰¹æ€§è°ƒæ•´å¤„ç†å‚æ•°
        
        Args:
            img (ndarray): è¾“å…¥å›¾åƒ
            lighting_analysis (dict): å…‰ç…§åˆ†æç»“æœï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨åˆ†æ
            conservative_mode (bool): ä¿å®ˆæ¨¡å¼ï¼Œå‡å°‘è¿‡åº¦å¤„ç†
            
        Returns:
            tuple: (å½’ä¸€åŒ–åçš„å›¾åƒ, å…‰ç…§åˆ†æç»“æœ)
        """
        if lighting_analysis is None:
            lighting_analysis = self.analyze_image_lighting(img)
        
        print(f"    å…‰ç…§ç±»å‹: {lighting_analysis['lighting_type']}, "
              f"å¯¹æ¯”åº¦: {lighting_analysis['contrast_type']}, "
              f"å¹³å‡äº®åº¦: {lighting_analysis['mean_brightness']:.1f}")
        
        # è½¬æ¢ä¸ºLABé¢œè‰²ç©ºé—´
        lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        
        # æ ¹æ®å…‰ç…§ç±»å‹è°ƒæ•´CLAHEå‚æ•° - ä¿å®ˆæ¨¡å¼å‡å°‘å¢å¼ºå¼ºåº¦
        if lighting_analysis['lighting_type'] == 'dark':
            # æš—å…‰å›¾åƒï¼šé€‚åº¦å¢å¼ºï¼Œé¿å…è¿‡åº¦å¤„ç†
            clip_limit = 3.0 if conservative_mode else 4.0
            tile_grid_size = (8, 8) if conservative_mode else (6, 6)
        elif lighting_analysis['lighting_type'] == 'bright':
            # äº®å…‰å›¾åƒï¼šè½»åº¦å¢å¼º
            clip_limit = 1.5 if conservative_mode else 2.0
            tile_grid_size = (12, 12) if conservative_mode else (10, 10)
        else:
            # æ­£å¸¸å…‰ç…§ï¼šæ ‡å‡†å‚æ•°
            clip_limit = 2.5 if conservative_mode else 3.0
            tile_grid_size = (8, 8)
        
        # æ ¹æ®å¯¹æ¯”åº¦ç±»å‹è¿›ä¸€æ­¥è°ƒæ•´ - ä¿å®ˆæ¨¡å¼å‡å°‘è°ƒæ•´å¹…åº¦
        if lighting_analysis['contrast_type'] == 'low':
            clip_limit += 0.5 if conservative_mode else 1.0
        elif lighting_analysis['contrast_type'] == 'high':
            clip_limit -= 0.3 if conservative_mode else 0.5
        
        # åº”ç”¨è‡ªé€‚åº”CLAHE
        clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)
        l_clahe = clahe.apply(l)
        
        # åˆå¹¶é€šé“
        lab_clahe = cv2.merge([l_clahe, a, b])
        
        # è½¬å›BGRé¢œè‰²ç©ºé—´
        normalized = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)
        
        # ä¿å®ˆæ¨¡å¼ä¸‹å‡å°‘å…¨å±€ç›´æ–¹å›¾å‡è¡¡åŒ–çš„ä½¿ç”¨
        if not conservative_mode and (lighting_analysis['contrast_type'] == 'low' or lighting_analysis['dynamic_range'] < 100):
            # åªæœ‰ä½å¯¹æ¯”åº¦æˆ–åŠ¨æ€èŒƒå›´å°çš„å›¾åƒæ‰åº”ç”¨å…¨å±€ç›´æ–¹å›¾å‡è¡¡åŒ–
            yuv = cv2.cvtColor(normalized, cv2.COLOR_BGR2YUV)
            yuv[:,:,0] = cv2.equalizeHist(yuv[:,:,0])
            normalized = cv2.cvtColor(yuv, cv2.COLOR_YUV2BGR)
        
        return normalized, lighting_analysis
    
    def normalize_image(self, img):
        """
        3.2.3.2 å½’ä¸€åŒ–å›¾åƒå¤„ç†
        å‡å°‘å…‰ç…§ã€å¤©æ°”ç­‰å› ç´ å¯¹å›¾åƒçš„å½±å“
        
        Args:
            img (ndarray): è¾“å…¥å›¾åƒ
            
        Returns:
            ndarray: å½’ä¸€åŒ–åçš„å›¾åƒ
        """
        normalized, _ = self.adaptive_normalize_image(img)
        return normalized
    
    def detect_building_contours(self, img):
        """
        3.2.4 å»ºç­‘ç‰©åŒºåŸŸè¯†åˆ«ï¼ˆä½¿ç”¨è½®å»“æ£€æµ‹æ›¿ä»£æ¨¡å‹æ£€æµ‹ï¼‰
        
        Args:
            img (ndarray): è¾“å…¥å›¾åƒ
            
        Returns:
            tuple: (è½®å»“åˆ—è¡¨, å»ºç­‘ç‰©åŒºåŸŸåˆ—è¡¨, å¤„ç†è¿‡ç¨‹å›¾åƒ)
        """
        print("  æ‰§è¡Œå»ºç­‘ç‰©è½®å»“æ£€æµ‹...")
        
        # è½¬æ¢ä¸ºç°åº¦å›¾
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        
        # é«˜æ–¯æ¨¡ç³Šå»å™ª
        blurred = cv2.GaussianBlur(gray, (5, 5), 0)
        
        # å¤šå±‚æ¬¡è¾¹ç¼˜æ£€æµ‹
        # 1. Cannyè¾¹ç¼˜æ£€æµ‹
        edges = cv2.Canny(blurred, 50, 150)
        
        # 2. å½¢æ€å­¦æ“ä½œè¿æ¥æ–­è£‚çš„è¾¹ç¼˜
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
        edges = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)
        edges = cv2.morphologyEx(edges, cv2.MORPH_DILATE, kernel)
        
        # 3. æŸ¥æ‰¾è½®å»“
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        # ç­›é€‰è½®å»“ï¼ˆå»ºç­‘ç‰©ç‰¹å¾ï¼‰
        building_contours = []
        building_regions = []
        
        for contour in contours:
            area = cv2.contourArea(contour)
            
            # é¢ç§¯ç­›é€‰ï¼šå¤§äºæœ€å°å»ºç­‘ç‰©å°ºå¯¸
            if area > self.contour_area_threshold:
                # è®¡ç®—è½®å»“çš„è¾¹ç•ŒçŸ©å½¢
                x, y, w, h = cv2.boundingRect(contour)
                
                # å°ºå¯¸ç­›é€‰ï¼šç¬¦åˆæœ€å°å»ºç­‘ç‰©å°ºå¯¸è¦æ±‚
                if w >= self.min_building_size[0] and h >= self.min_building_size[1]:
                    # é•¿å®½æ¯”ç­›é€‰ï¼šæ’é™¤è¿‡äºç»†é•¿çš„åŒºåŸŸ
                    aspect_ratio = max(w, h) / min(w, h)
                    if aspect_ratio <= 5.0:  # é•¿å®½æ¯”ä¸è¶…è¿‡5:1
                        # è½®å»“å¤æ‚åº¦ç­›é€‰ï¼šå»ºç­‘ç‰©é€šå¸¸æœ‰ä¸€å®šçš„å¤æ‚åº¦
                        perimeter = cv2.arcLength(contour, True)
                        if perimeter > 0:
                            circularity = 4 * np.pi * area / (perimeter * perimeter)
                            if 0.1 <= circularity <= 0.9:  # æ’é™¤è¿‡äºåœ†å½¢æˆ–è¿‡äºå¤æ‚çš„å½¢çŠ¶
                                building_contours.append(contour)
                                building_regions.append({
                                    'bbox': (x, y, w, h),
                                    'area': area,
                                    'perimeter': perimeter,
                                    'aspect_ratio': aspect_ratio,
                                    'circularity': circularity,
                                    'contour': contour
                                })
        
        print(f"    æ£€æµ‹åˆ° {len(contours)} ä¸ªæ€»è½®å»“")
        print(f"    ç­›é€‰å‡º {len(building_contours)} ä¸ªå»ºç­‘ç‰©è½®å»“")
        
        # åˆ›å»ºå¤„ç†è¿‡ç¨‹å¯è§†åŒ–
        process_img = img.copy()
        
        # ç»˜åˆ¶æ‰€æœ‰è½®å»“ï¼ˆç°è‰²ï¼‰
        cv2.drawContours(process_img, contours, -1, (128, 128, 128), 1)
        
        # ç»˜åˆ¶å»ºç­‘ç‰©è½®å»“ï¼ˆçº¢è‰²ï¼‰
        cv2.drawContours(process_img, building_contours, -1, (0, 0, 255), 2)
        
        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        for region in building_regions:
            x, y, w, h = region['bbox']
            cv2.rectangle(process_img, (x, y), (x+w, y+h), (255, 0, 0), 2)
            # æ·»åŠ é¢ç§¯æ ‡ç­¾
            cv2.putText(process_img, f"{int(region['area'])}", 
                       (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)
        
        return building_contours, building_regions, process_img
    
    def calculate_adaptive_threshold(self, diff_img, conservative_factor=1.5):
        """
        è®¡ç®—è‡ªé€‚åº”é˜ˆå€¼
        
        Args:
            diff_img (ndarray): å·®å¼‚å›¾åƒ
            conservative_factor (float): ä¿å®ˆç³»æ•°ï¼Œè¶Šå¤§è¶Šä¿å®ˆ
            
        Returns:
            int: è‡ªé€‚åº”é˜ˆå€¼
        """
        mean_diff = np.mean(diff_img)
        std_diff = np.std(diff_img)
        
        # ä½¿ç”¨ç»Ÿè®¡æ–¹æ³•è®¡ç®—é˜ˆå€¼ï¼šå‡å€¼ + ä¿å®ˆç³»æ•° * æ ‡å‡†å·®
        adaptive_threshold = int(mean_diff + conservative_factor * std_diff)
        
        # é™åˆ¶é˜ˆå€¼èŒƒå›´
        adaptive_threshold = max(30, min(adaptive_threshold, 120))
        
        return adaptive_threshold
    
    def region_pixel_comparison(self, img1, img2, regions1, regions2):
        """
        3.2.5 åŒºåŸŸåƒç´ å·®æ¯”è¾ƒ
        
        Args:
            img1 (ndarray): ç¬¬ä¸€æœŸå›¾åƒ
            img2 (ndarray): ç¬¬äºŒæœŸå›¾åƒ
            regions1 (list): ç¬¬ä¸€æœŸå»ºç­‘ç‰©åŒºåŸŸ
            regions2 (list): ç¬¬äºŒæœŸå»ºç­‘ç‰©åŒºåŸŸ
            
        Returns:
            dict: åƒç´ å·®æ¯”è¾ƒç»“æœ
        """
        print("  æ‰§è¡ŒåŒºåŸŸåƒç´ å·®æ¯”è¾ƒ...")
        
        # è½¬æ¢ä¸ºç°åº¦å›¾è¿›è¡Œåƒç´ å·®è®¡ç®—
        gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
        gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
        
        # ç¡®ä¿å›¾åƒå°ºå¯¸ä¸€è‡´
        if gray1.shape != gray2.shape:
            gray2 = cv2.resize(gray2, (gray1.shape[1], gray1.shape[0]))
        
        # è®¡ç®—æ•´ä½“åƒç´ å·®
        diff_img = cv2.absdiff(gray1, gray2)
        
        # è®¡ç®—è‡ªé€‚åº”é˜ˆå€¼
        if self.adaptive_threshold:
            current_threshold = self.calculate_adaptive_threshold(diff_img)
            print(f"    è‡ªé€‚åº”é˜ˆå€¼: {current_threshold} (åŸå§‹é˜ˆå€¼: {self.pixel_diff_threshold})")
        else:
            current_threshold = self.pixel_diff_threshold
        
        # åŒºåŸŸåŒ¹é…å’Œæ¯”è¾ƒ
        region_comparisons = []
        suspicious_regions = []
        
        # ç®€åŒ–çš„åŒºåŸŸåŒ¹é…ï¼šåŸºäºä½ç½®è·ç¦»
        matched_pairs = self._match_regions(regions1, regions2)
        
        print(f"    åŒ¹é…åˆ° {len(matched_pairs)} å¯¹åŒºåŸŸ")
        
        for i, (region1, region2) in enumerate(matched_pairs):
            # æå–åŒºåŸŸåƒç´ 
            x1, y1, w1, h1 = region1['bbox']
            x2, y2, w2, h2 = region2['bbox']
            
            # ä½¿ç”¨è¾ƒå¤§çš„è¾¹ç•Œæ¡†ç¡®ä¿è¦†ç›–å®Œæ•´åŒºåŸŸ
            x = min(x1, x2)
            y = min(y1, y2)
            w = max(x1 + w1, x2 + w2) - x
            h = max(y1 + h1, y2 + h2) - y
            
            # ç¡®ä¿åŒºåŸŸåœ¨å›¾åƒèŒƒå›´å†…
            x = max(0, x)
            y = max(0, y)
            w = min(w, gray1.shape[1] - x)
            h = min(h, gray1.shape[0] - y)
            
            if w <= 0 or h <= 0:
                continue
            
            # æå–å¯¹åº”åŒºåŸŸ
            region_gray1 = gray1[y:y+h, x:x+w]
            region_gray2 = gray2[y:y+h, x:x+w]
            region_diff = diff_img[y:y+h, x:x+w]
            
            # è®¡ç®—åŒºåŸŸç»Ÿè®¡ä¿¡æ¯
            mean_diff = np.mean(region_diff)
            max_diff = np.max(region_diff)
            std_diff = np.std(region_diff)
            
            # è®¡ç®—è¶…è¿‡é˜ˆå€¼çš„åƒç´ æ¯”ä¾‹
            threshold_pixels = np.sum(region_diff > current_threshold)
            total_pixels = region_diff.size
            threshold_ratio = threshold_pixels / total_pixels if total_pixels > 0 else 0
            
            region_comparison = {
                'region_id': i + 1,
                'bbox': (x, y, w, h),
                'region1_info': region1,
                'region2_info': region2,
                'mean_diff': float(mean_diff),
                'max_diff': float(max_diff),
                'std_diff': float(std_diff),
                'threshold_pixels': int(threshold_pixels),
                'total_pixels': int(total_pixels),
                'threshold_ratio': float(threshold_ratio),
                'is_suspicious': threshold_ratio > 0.3  # 30%ä»¥ä¸Šåƒç´ å·®å¼‚è¶…è¿‡é˜ˆå€¼åˆ™æ ‡è®°ä¸ºç–‘ä¼¼
            }
            
            region_comparisons.append(region_comparison)
            
            # æ ‡è®°ç–‘ä¼¼è¿ç« å»ºç­‘åŒºåŸŸ
            if region_comparison['is_suspicious']:
                suspicious_regions.append(region_comparison)
        
        # æ£€æŸ¥æ–°å¢åŒºåŸŸï¼ˆåªåœ¨ç¬¬äºŒæœŸå‡ºç°çš„å»ºç­‘ç‰©ï¼‰
        unmatched_regions2 = self._find_unmatched_regions(regions2, matched_pairs, is_second_period=True)
        for i, region in enumerate(unmatched_regions2):
            x, y, w, h = region['bbox']
            
            # æå–åŒºåŸŸå·®å¼‚
            region_diff = diff_img[y:y+h, x:x+w]
            mean_diff = np.mean(region_diff)
            threshold_pixels = np.sum(region_diff > current_threshold)
            total_pixels = region_diff.size
            threshold_ratio = threshold_pixels / total_pixels if total_pixels > 0 else 0
            
            new_building = {
                'region_id': f"new_{i + 1}",
                'bbox': (x, y, w, h),
                'region1_info': None,
                'region2_info': region,
                'mean_diff': float(mean_diff),
                'max_diff': 255.0,  # æ–°å»ºç­‘ç‰©ï¼Œæœ€å¤§å·®å¼‚
                'std_diff': float(np.std(region_diff)),
                'threshold_pixels': int(threshold_pixels),
                'total_pixels': int(total_pixels),
                'threshold_ratio': float(threshold_ratio),
                'is_suspicious': True,  # æ–°å»ºç­‘ç‰©ç›´æ¥æ ‡è®°ä¸ºç–‘ä¼¼
                'is_new_building': True
            }
            
            region_comparisons.append(new_building)
            suspicious_regions.append(new_building)
        
        print(f"    å‘ç° {len(suspicious_regions)} ä¸ªç–‘ä¼¼è¿ç« å»ºç­‘åŒºåŸŸ")
        
        return {
            'diff_image': diff_img,
            'region_comparisons': region_comparisons,
            'suspicious_regions': suspicious_regions,
            'total_regions_compared': len(matched_pairs),
            'new_buildings_detected': len(unmatched_regions2),
            'suspicious_count': len(suspicious_regions)
        }
    
    def _match_regions(self, regions1, regions2, distance_threshold=100):
        """
        åŸºäºä½ç½®è·ç¦»åŒ¹é…ä¸¤æœŸå›¾åƒä¸­çš„å»ºç­‘ç‰©åŒºåŸŸ
        
        Args:
            regions1 (list): ç¬¬ä¸€æœŸåŒºåŸŸ
            regions2 (list): ç¬¬äºŒæœŸåŒºåŸŸ
            distance_threshold (float): è·ç¦»é˜ˆå€¼
            
        Returns:
            list: åŒ¹é…çš„åŒºåŸŸå¯¹åˆ—è¡¨
        """
        matched_pairs = []
        used_indices2 = set()
        
        for region1 in regions1:
            x1, y1, w1, h1 = region1['bbox']
            center1 = (x1 + w1/2, y1 + h1/2)
            
            best_match = None
            best_distance = float('inf')
            best_index = -1
            
            for i, region2 in enumerate(regions2):
                if i in used_indices2:
                    continue
                
                x2, y2, w2, h2 = region2['bbox']
                center2 = (x2 + w2/2, y2 + h2/2)
                
                # è®¡ç®—ä¸­å¿ƒç‚¹è·ç¦»
                distance = np.sqrt((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)
                
                if distance < distance_threshold and distance < best_distance:
                    best_distance = distance
                    best_match = region2
                    best_index = i
            
            if best_match is not None:
                matched_pairs.append((region1, best_match))
                used_indices2.add(best_index)
        
        return matched_pairs
    
    def _find_unmatched_regions(self, regions, matched_pairs, is_second_period=False):
        """
        æ‰¾åˆ°æœªåŒ¹é…çš„åŒºåŸŸ
        
        Args:
            regions (list): åŒºåŸŸåˆ—è¡¨
            matched_pairs (list): åŒ¹é…å¯¹åˆ—è¡¨
            is_second_period (bool): æ˜¯å¦ä¸ºç¬¬äºŒæœŸå›¾åƒ
            
        Returns:
            list: æœªåŒ¹é…çš„åŒºåŸŸåˆ—è¡¨
        """
        if is_second_period:
            matched_regions = [pair[1] for pair in matched_pairs]
        else:
            matched_regions = [pair[0] for pair in matched_pairs]
        
        unmatched = []
        for region in regions:
            if region not in matched_regions:
                unmatched.append(region)
        
        return unmatched
    
    def create_detection_visualization(self, img1, img2, img1_processed, img2_processed, 
                                     comparison_result, transform_info, comparison_name):
        """
        åˆ›å»ºè¿ç« å»ºç­‘æ£€æµ‹å¯è§†åŒ–å›¾åƒ
        
        Args:
            img1, img2: åŸå§‹å›¾åƒ
            img1_processed, img2_processed: å¤„ç†åå›¾åƒ
            comparison_result: æ¯”è¾ƒç»“æœ
            transform_info: å˜æ¢ä¿¡æ¯
            comparison_name: æ¯”è¾ƒåç§°
            
        Returns:
            str: ä¿å­˜çš„å›¾åƒè·¯å¾„
        """
        fig, axes = plt.subplots(3, 5, figsize=(25, 15))
        fig.suptitle(f'è¿ç« å»ºç­‘æ£€æµ‹åˆ†æ - {comparison_name}', fontsize=16, fontweight='bold')
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # ç¬¬ä¸€è¡Œï¼šåŸå§‹å›¾åƒå’Œé€è§†æ ¡æ­£
        axes[0, 0].imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))
        axes[0, 0].set_title('Base Image\nç¬¬ä¸€æœŸå›¾åƒï¼ˆåŸºå‡†ï¼‰', fontsize=12)
        axes[0, 0].axis('off')
        
        axes[0, 1].imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))
        axes[0, 1].set_title('Original Image\nç¬¬äºŒæœŸå›¾åƒï¼ˆåŸå§‹ï¼‰', fontsize=12)
        axes[0, 1].axis('off')
        
        axes[0, 2].imshow(cv2.cvtColor(img2_processed, cv2.COLOR_BGR2RGB))
        axes[0, 2].set_title(f'Aligned Image\né€è§†æ ¡æ­£å\n(å†…ç‚¹ç‡: {transform_info.get("inlier_ratio", 0):.2%})', fontsize=11)
        axes[0, 2].axis('off')
        
        # åƒç´ å·®å¼‚å›¾ï¼ˆçƒ­å›¾é£æ ¼ï¼‰
        diff_img = comparison_result['diff_image']
        axes[0, 3].imshow(diff_img, cmap='hot')
        axes[0, 3].set_title(f'çƒ­å›¾å·®å¼‚\n(é˜ˆå€¼: {self.pixel_diff_threshold})', fontsize=11)
        axes[0, 3].axis('off')
        
        # æ·»åŠ  Difference Mapï¼ˆè“è‰²é£æ ¼ï¼‰
        # åˆ›å»ºå¢å¼ºçš„difference map
        enhanced_diff = self._create_enhanced_difference_map(diff_img)
        im = axes[0, 4].imshow(enhanced_diff, cmap='Blues')
        axes[0, 4].set_title('Difference Map\nå·®å¼‚æ˜ å°„å›¾', fontsize=11)
        axes[0, 4].axis('off')
        
        # æ·»åŠ é¢œè‰²æ¡
        plt.colorbar(im, ax=axes[0, 4], fraction=0.046, pad=0.04)
        
        # ç¬¬äºŒè¡Œï¼šå»ºç­‘ç‰©æ£€æµ‹ç»“æœ
        # ç»˜åˆ¶ç¬¬ä¸€æœŸå»ºç­‘ç‰©æ£€æµ‹
        img1_buildings = img1_processed.copy()
        for region in comparison_result['region_comparisons']:
            if region['region1_info'] is not None:
                x, y, w, h = region['region1_info']['bbox']
                cv2.rectangle(img1_buildings, (x, y), (x+w, y+h), (0, 255, 0), 2)
                cv2.putText(img1_buildings, f"B{region['region_id']}", 
                           (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        
        axes[1, 0].imshow(cv2.cvtColor(img1_buildings, cv2.COLOR_BGR2RGB))
        axes[1, 0].set_title(f'ç¬¬ä¸€æœŸå»ºç­‘ç‰©æ£€æµ‹\n(æ£€æµ‹æ•°é‡: {len([r for r in comparison_result["region_comparisons"] if r["region1_info"] is not None])})', fontsize=11)
        axes[1, 0].axis('off')
        
        # ç»˜åˆ¶ç¬¬äºŒæœŸå»ºç­‘ç‰©æ£€æµ‹
        img2_buildings = img2_processed.copy()
        for region in comparison_result['region_comparisons']:
            if region['region2_info'] is not None:
                x, y, w, h = region['region2_info']['bbox']
                color = (0, 0, 255) if region['is_suspicious'] else (0, 255, 0)
                cv2.rectangle(img2_buildings, (x, y), (x+w, y+h), color, 2)
                label = f"S{region['region_id']}" if region['is_suspicious'] else f"B{region['region_id']}"
                cv2.putText(img2_buildings, label, 
                           (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
        
        axes[1, 1].imshow(cv2.cvtColor(img2_buildings, cv2.COLOR_BGR2RGB))
        axes[1, 1].set_title(f'ç¬¬äºŒæœŸå»ºç­‘ç‰©æ£€æµ‹\n(ç–‘ä¼¼: {comparison_result["suspicious_count"]})', fontsize=11)
        axes[1, 1].axis('off')
        
        # ç–‘ä¼¼è¿ç« å»ºç­‘åŒºåŸŸæ”¾å¤§æ˜¾ç¤º
        if comparison_result['suspicious_regions']:
            # é€‰æ‹©ç¬¬ä¸€ä¸ªç–‘ä¼¼åŒºåŸŸè¿›è¡Œæ”¾å¤§æ˜¾ç¤º
            suspicious = comparison_result['suspicious_regions'][0]
            x, y, w, h = suspicious['bbox']
            
            # æ·»åŠ è¾¹è·
            margin = 20
            x1 = max(0, x - margin)
            y1 = max(0, y - margin)
            x2 = min(img2_processed.shape[1], x + w + margin)
            y2 = min(img2_processed.shape[0], y + h + margin)
            
            cropped_region = img2_processed[y1:y2, x1:x2]
            axes[1, 2].imshow(cv2.cvtColor(cropped_region, cv2.COLOR_BGR2RGB))
            axes[1, 2].set_title(f'ç–‘ä¼¼åŒºåŸŸæ”¾å¤§\n(å·®å¼‚ç‡: {suspicious["threshold_ratio"]:.2%})', fontsize=11)
            axes[1, 2].axis('off')
        else:
            axes[1, 2].text(0.5, 0.5, 'æ— ç–‘ä¼¼åŒºåŸŸ', ha='center', va='center', 
                           transform=axes[1, 2].transAxes, fontsize=14)
            axes[1, 2].set_title('ç–‘ä¼¼åŒºåŸŸ', fontsize=11)
            axes[1, 2].axis('off')
        
        # å·®å¼‚ç»Ÿè®¡ç›´æ–¹å›¾
        diff_flat = diff_img.flatten()
        axes[1, 3].hist(diff_flat, bins=50, alpha=0.7, color='red')
        axes[1, 3].axvline(x=self.pixel_diff_threshold, color='black', linestyle='--', 
                          label=f'é˜ˆå€¼: {self.pixel_diff_threshold}')
        axes[1, 3].set_title('åƒç´ å·®å¼‚åˆ†å¸ƒ', fontsize=11)
        axes[1, 3].set_xlabel('åƒç´ å·®å€¼')
        axes[1, 3].set_ylabel('é¢‘æ¬¡')
        axes[1, 3].legend()
        
        # æ·»åŠ è¾¹ç¼˜è½®å»“å¯¹æ¯”å›¾
        contour_comparison = self._create_contour_comparison(img1_processed, img2_processed, diff_img)
        axes[1, 4].imshow(cv2.cvtColor(contour_comparison, cv2.COLOR_BGR2RGB))
        axes[1, 4].set_title('è½®å»“å¯¹æ¯”\nContour Comparison', fontsize=11)
        axes[1, 4].axis('off')
        
        # ç¬¬ä¸‰è¡Œï¼šç»Ÿè®¡ä¿¡æ¯å’Œç³»ç»Ÿå‚æ•°
        self._draw_detection_statistics(axes[2, 0], comparison_result)
        self._draw_system_parameters(axes[2, 1])
        self._draw_region_analysis(axes[2, 2], comparison_result)
        self._draw_technical_process(axes[2, 3])
        self._draw_difference_analysis(axes[2, 4], comparison_result)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾åƒ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_name = "".join(c for c in comparison_name if c.isalnum() or c in (' ', '-', '_')).rstrip()
        image_path = os.path.join(self.output_dir, f"illegal_building_detection_{safe_name}_{timestamp}.jpg")
        
        plt.savefig(image_path, dpi=150, bbox_inches='tight', facecolor='white')
        plt.close()
        
        return image_path
    
    def _draw_detection_statistics(self, ax, result):
        """ç»˜åˆ¶æ£€æµ‹ç»Ÿè®¡ä¿¡æ¯"""
        stats_text = f"""ğŸ¢ æ£€æµ‹ç»Ÿè®¡ä¿¡æ¯

æ€»åŒºåŸŸå¯¹æ¯”: {result['total_regions_compared']}
æ–°å»ºç­‘æ£€æµ‹: {result['new_buildings_detected']}
ç–‘ä¼¼è¿ç« : {result['suspicious_count']}

ğŸ“Š åƒç´ å·®å¼‚ç»Ÿè®¡:
å¹³å‡å·®å¼‚: {np.mean(result['diff_image']):.2f}
æœ€å¤§å·®å¼‚: {np.max(result['diff_image']):.2f}
æ ‡å‡†å·®: {np.std(result['diff_image']):.2f}

ğŸ¯ é˜ˆå€¼è®¾ç½®:
åƒç´ å·®é˜ˆå€¼: {self.pixel_diff_threshold}
ç–‘ä¼¼åˆ¤å®š: 30%åƒç´ è¶…é˜ˆå€¼
"""
        
        ax.text(0.05, 0.95, stats_text, ha='left', va='top', transform=ax.transAxes,
                fontsize=10, bbox=dict(boxstyle='round,pad=0.3', facecolor='lightblue', alpha=0.8))
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.axis('off')
        ax.set_title('æ£€æµ‹ç»Ÿè®¡', fontsize=12, fontweight='bold')
    
    def _draw_system_parameters(self, ax):
        """ç»˜åˆ¶ç³»ç»Ÿå‚æ•°"""
        params_text = f"""âš™ï¸ ç³»ç»Ÿå‚æ•°é…ç½®

ğŸ“· å›¾åƒè¦æ±‚:
åˆ†è¾¨ç‡: â‰¥1080P
ç›®æ ‡å°ºå¯¸: â‰¥50Ã—50px
å…‰ç…§èŒƒå›´: {self.illumination_range[0]}-{self.illumination_range[1]} lux

ğŸ”§ æ£€æµ‹å‚æ•°:
æœ€å°å»ºç­‘å°ºå¯¸: {self.min_building_size}
è½®å»“é¢ç§¯é˜ˆå€¼: {self.contour_area_threshold}
åƒç´ å·®é˜ˆå€¼: {self.pixel_diff_threshold}
é•¿å®½æ¯”é™åˆ¶: â‰¤5:1
åœ†å½¢åº¦èŒƒå›´: 0.1-0.9

ğŸ“ æŠ€æœ¯æ ‡å‡†:
ç¬¦åˆè¿å»ºæ£€æµ‹è§„èŒƒ
æ»¡è¶³å®æ—¶å¤„ç†è¦æ±‚
"""
        
        ax.text(0.05, 0.95, params_text, ha='left', va='top', transform=ax.transAxes,
                fontsize=9, bbox=dict(boxstyle='round,pad=0.3', facecolor='lightgreen', alpha=0.8))
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.axis('off')
        ax.set_title('ç³»ç»Ÿå‚æ•°', fontsize=12, fontweight='bold')
    
    def _draw_region_analysis(self, ax, result):
        """ç»˜åˆ¶åŒºåŸŸåˆ†æ"""
        if not result['suspicious_regions']:
            ax.text(0.5, 0.5, 'æš‚æ— ç–‘ä¼¼åŒºåŸŸ', ha='center', va='center', 
                   transform=ax.transAxes, fontsize=14)
        else:
            # æ˜¾ç¤ºç–‘ä¼¼åŒºåŸŸè¯¦æƒ…
            suspicious = result['suspicious_regions'][0]  # æ˜¾ç¤ºç¬¬ä¸€ä¸ªç–‘ä¼¼åŒºåŸŸ
            
            analysis_text = f"""ğŸš¨ ç–‘ä¼¼åŒºåŸŸåˆ†æ

åŒºåŸŸID: {suspicious['region_id']}
ä½ç½®: ({suspicious['bbox'][0]}, {suspicious['bbox'][1]})
å°ºå¯¸: {suspicious['bbox'][2]}Ã—{suspicious['bbox'][3]}

åƒç´ å·®å¼‚åˆ†æ:
å¹³å‡å·®å¼‚: {suspicious['mean_diff']:.2f}
æœ€å¤§å·®å¼‚: {suspicious['max_diff']:.2f}
æ ‡å‡†å·®: {suspicious['std_diff']:.2f}

é˜ˆå€¼åˆ†æ:
è¶…é˜ˆå€¼åƒç´ : {suspicious['threshold_pixels']}
æ€»åƒç´ æ•°: {suspicious['total_pixels']}
è¶…é˜ˆå€¼æ¯”ä¾‹: {suspicious['threshold_ratio']:.2%}

åˆ¤å®šç»“æœ: {'ç–‘ä¼¼è¿ç« å»ºç­‘' if suspicious['is_suspicious'] else 'æ­£å¸¸å»ºç­‘'}
"""
            
            color = 'lightcoral' if suspicious['is_suspicious'] else 'lightgreen'
            ax.text(0.05, 0.95, analysis_text, ha='left', va='top', transform=ax.transAxes,
                    fontsize=9, bbox=dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.8))
        
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.axis('off')
        ax.set_title('åŒºåŸŸåˆ†æ', fontsize=12, fontweight='bold')
    
    def _draw_technical_process(self, ax):
        """ç»˜åˆ¶æŠ€æœ¯æµç¨‹"""
        process_text = """ğŸ”¬ æŠ€æœ¯å¤„ç†æµç¨‹

1ï¸âƒ£ å›¾åƒé¢„å¤„ç†
â€¢ é€è§†å˜æ¢çŸ©é˜µæ ¡æ­£
â€¢ CLAHEå…‰ç…§å½’ä¸€åŒ–
â€¢ ç›´æ–¹å›¾å‡è¡¡åŒ–

2ï¸âƒ£ å»ºç­‘ç‰©æ£€æµ‹
â€¢ Cannyè¾¹ç¼˜æ£€æµ‹
â€¢ å½¢æ€å­¦æ“ä½œ
â€¢ è½®å»“ç­›é€‰è¿‡æ»¤

3ï¸âƒ£ åŒºåŸŸåŒ¹é…
â€¢ åŸºäºä½ç½®è·ç¦»åŒ¹é…
â€¢ å‡ ä½•ç‰¹å¾éªŒè¯
â€¢ æ–°å»ºç­‘ç‰©è¯†åˆ«

4ï¸âƒ£ åƒç´ å·®æ¯”è¾ƒ
â€¢ é€åƒç´ å·®å€¼è®¡ç®—
â€¢ é˜ˆå€¼æ¯”ä¾‹åˆ†æ
â€¢ ç–‘ä¼¼åŒºåŸŸæ ‡è®°

âœ… ç¬¦åˆæŠ€æœ¯è§„èŒƒè¦æ±‚"""
        
        ax.text(0.05, 0.95, process_text, ha='left', va='top', transform=ax.transAxes,
                fontsize=9, bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow', alpha=0.8))
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.axis('off')
        ax.set_title('æŠ€æœ¯æµç¨‹', fontsize=12, fontweight='bold')
    
    def _create_enhanced_difference_map(self, diff_img):
        """
        åˆ›å»ºå¢å¼ºçš„difference map
        
        Args:
            diff_img (ndarray): åŸå§‹å·®å¼‚å›¾
            
        Returns:
            ndarray: å¢å¼ºçš„å·®å¼‚å›¾
        """
        # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
        normalized_diff = diff_img.astype(np.float32) / 255.0
        
        # åº”ç”¨éçº¿æ€§å¢å¼º
        enhanced_diff = np.power(normalized_diff, 0.5)  # å¼€æ–¹å¢å¼ºå¯¹æ¯”åº¦
        
        # åº”ç”¨é«˜æ–¯æ»¤æ³¢å¹³æ»‘å¤„ç†
        enhanced_diff = cv2.GaussianBlur(enhanced_diff, (3, 3), 0)
        
        # é˜ˆå€¼åŒ–å¤„ç†ï¼Œçªå‡ºæ˜¾è‘—å·®å¼‚åŒºåŸŸ
        threshold_mask = diff_img > self.pixel_diff_threshold
        enhanced_diff[threshold_mask] = enhanced_diff[threshold_mask] * 1.5
        enhanced_diff = np.clip(enhanced_diff, 0, 1)
        
        return enhanced_diff
    
    def _create_contour_comparison(self, img1, img2, diff_img):
        """
        åˆ›å»ºè½®å»“å¯¹æ¯”å›¾
        
        Args:
            img1 (ndarray): ç¬¬ä¸€æœŸå›¾åƒ
            img2 (ndarray): ç¬¬äºŒæœŸå›¾åƒ
            diff_img (ndarray): å·®å¼‚å›¾åƒ
            
        Returns:
            ndarray: è½®å»“å¯¹æ¯”å›¾
        """
        # åˆ›å»ºç»„åˆå›¾åƒ
        h, w = img1.shape[:2]
        comparison_img = np.zeros((h, w, 3), dtype=np.uint8)
        
        # å°†ç¬¬ä¸€æœŸå›¾åƒè½¬ä¸ºçº¢è‰²é€šé“
        gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
        comparison_img[:, :, 2] = gray1  # çº¢è‰²é€šé“
        
        # å°†ç¬¬äºŒæœŸå›¾åƒè½¬ä¸ºç»¿è‰²é€šé“
        gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
        comparison_img[:, :, 1] = gray2  # ç»¿è‰²é€šé“
        
        # å°†å·®å¼‚åŒºåŸŸè½¬ä¸ºè“è‰²é€šé“
        diff_threshold = diff_img > self.pixel_diff_threshold
        comparison_img[:, :, 0] = diff_img  # è“è‰²é€šé“
        
        # å¢å¼ºå·®å¼‚åŒºåŸŸçš„å¯è§æ€§
        comparison_img[diff_threshold] = [255, 100, 100]  # æµ…è“è‰²æ ‡è®°å·®å¼‚åŒºåŸŸ
        
        return comparison_img
    
    def _draw_difference_analysis(self, ax, result):
        """ç»˜åˆ¶å·®å¼‚åˆ†æä¿¡æ¯"""
        diff_img = result['diff_image']
        
        # è®¡ç®—å·®å¼‚ç»Ÿè®¡
        total_pixels = diff_img.size
        significant_diff_pixels = np.sum(diff_img > self.pixel_diff_threshold)
        diff_percentage = (significant_diff_pixels / total_pixels) * 100
        
        mean_diff = np.mean(diff_img)
        max_diff = np.max(diff_img)
        std_diff = np.std(diff_img)
        
        analysis_text = f"""ğŸ“Š å·®å¼‚åˆ†æç»Ÿè®¡
        
ğŸ” æ•´ä½“å·®å¼‚åˆ†æ:
åƒç´ æ€»æ•°: {total_pixels:,}
æ˜¾è‘—å·®å¼‚åƒç´ : {significant_diff_pixels:,}
å·®å¼‚æ¯”ä¾‹: {diff_percentage:.2f}%

ğŸ“ˆ ç»Ÿè®¡æŒ‡æ ‡:
å¹³å‡å·®å¼‚: {mean_diff:.2f}
æœ€å¤§å·®å¼‚: {max_diff:.2f}
æ ‡å‡†å·®: {std_diff:.2f}

ğŸ¯ é˜ˆå€¼è®¾ç½®:
å·®å¼‚é˜ˆå€¼: {self.pixel_diff_threshold}
ç–‘ä¼¼åˆ¤å®š: 30%åƒç´ è¶…é˜ˆå€¼

ğŸ“‹ ç»“æœè¯„ä¼°:
æ£€æµ‹ç²¾åº¦: {'é«˜' if diff_percentage > 5 else 'ä¸­' if diff_percentage > 1 else 'ä½'}
å˜åŒ–ç¨‹åº¦: {'æ˜¾è‘—' if mean_diff > 30 else 'ä¸­ç­‰' if mean_diff > 15 else 'è½»å¾®'}
"""
        
        color = 'lightcoral' if diff_percentage > 5 else 'lightyellow' if diff_percentage > 1 else 'lightgreen'
        ax.text(0.05, 0.95, analysis_text, ha='left', va='top', transform=ax.transAxes,
                fontsize=9, bbox=dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.8))
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.axis('off')
        ax.set_title('å·®å¼‚åˆ†æ', fontsize=12, fontweight='bold')
    
    def process_illegal_building_detection(self, img1_path, img2_path):
        """
        å¤„ç†è¿ç« å»ºç­‘æ£€æµ‹çš„å®Œæ•´æµç¨‹
        
        Args:
            img1_path (str): ç¬¬ä¸€æœŸå›¾åƒè·¯å¾„
            img2_path (str): ç¬¬äºŒæœŸå›¾åƒè·¯å¾„
            
        Returns:
            dict: æ£€æµ‹ç»“æœ
        """
        comparison_name = f"{os.path.basename(img1_path)}_vs_{os.path.basename(img2_path)}"
        print(f"\n=== è¿ç« å»ºç­‘æ£€æµ‹åˆ†æ: {comparison_name} ===")
        
        # 1. åŠ è½½å›¾åƒ
        print("1. åŠ è½½å›¾åƒ...")
        img1 = self.load_image_with_chinese_path(img1_path)
        img2 = self.load_image_with_chinese_path(img2_path)
        
        if img1 is None or img2 is None:
            return None
        
        print(f"   ç¬¬ä¸€æœŸå›¾åƒå°ºå¯¸: {img1.shape}")
        print(f"   ç¬¬äºŒæœŸå›¾åƒå°ºå¯¸: {img2.shape}")
        
        # 2. é€è§†å˜æ¢æ ¡æ­£
        print("2. é€è§†å˜æ¢æ ¡æ­£...")
        base_img, aligned_img, transform_matrix, transform_info = self.perspective_correction(img1, img2)
        
        # 3. è‡ªé€‚åº”å›¾åƒå½’ä¸€åŒ–
        print("3. è‡ªé€‚åº”å›¾åƒå½’ä¸€åŒ–å¤„ç†...")
        print("  åˆ†æç¬¬ä¸€æœŸå›¾åƒå…‰ç…§ç‰¹æ€§...")
        normalized_img1, lighting_info1 = self.adaptive_normalize_image(base_img, conservative_mode=True)
        
        print("  åˆ†æç¬¬äºŒæœŸå›¾åƒå…‰ç…§ç‰¹æ€§...")
        normalized_img2, lighting_info2 = self.adaptive_normalize_image(aligned_img, conservative_mode=True)
        
        # 4. å»ºç­‘ç‰©è½®å»“æ£€æµ‹
        print("4. å»ºç­‘ç‰©è½®å»“æ£€æµ‹...")
        contours1, regions1, process_img1 = self.detect_building_contours(normalized_img1)
        contours2, regions2, process_img2 = self.detect_building_contours(normalized_img2)
        
        # 5. åŒºåŸŸåƒç´ å·®æ¯”è¾ƒ
        print("5. åŒºåŸŸåƒç´ å·®æ¯”è¾ƒ...")
        comparison_result = self.region_pixel_comparison(normalized_img1, normalized_img2, regions1, regions2)
        
        # 6. ç”Ÿæˆå¯è§†åŒ–
        print("6. ç”Ÿæˆæ£€æµ‹å¯è§†åŒ–...")
        visualization_path = self.create_detection_visualization(
            img1, img2, normalized_img1, normalized_img2,
            comparison_result, transform_info, comparison_name
        )
        
        # 7. æ•´ç†ç»“æœ
        result = {
            'analysis_info': {
                'image1_path': img1_path,
                'image2_path': img2_path,
                'comparison_name': comparison_name,
                'timestamp': datetime.now().isoformat(),
                'image1_size': img1.shape,
                'image2_size': img2.shape
            },
            'preprocessing': {
                'perspective_correction': transform_info,
                'normalization_applied': True,
                'lighting_analysis': {
                    'period1': lighting_info1,
                    'period2': lighting_info2
                }
            },
            'building_detection': {
                'period1_buildings': len(regions1),
                'period2_buildings': len(regions2),
                'period1_contours': len(contours1),
                'period2_contours': len(contours2)
            },
            'pixel_comparison': comparison_result,
            'system_parameters': {
                'min_building_size': self.min_building_size,
                'pixel_diff_threshold': self.pixel_diff_threshold,
                'contour_area_threshold': self.contour_area_threshold,
                'illumination_range': self.illumination_range
            },
            'visualization_path': visualization_path
        }
        
        print(f"âœ… æ£€æµ‹å®Œæˆï¼å‘ç° {comparison_result['suspicious_count']} ä¸ªç–‘ä¼¼è¿ç« å»ºç­‘åŒºåŸŸ")
        return result
    
    def save_detection_results(self, results):
        """ä¿å­˜æ£€æµ‹ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜JSONç»“æœ
        json_path = os.path.join(self.output_dir, f"illegal_building_detection_results_{timestamp}.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
            serializable_results = self._make_json_serializable(results)
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        print(f"æ£€æµ‹ç»“æœå·²ä¿å­˜: {json_path}")
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self._generate_detection_report(results, timestamp)
        
        return json_path
    
    def _make_json_serializable(self, obj):
        """ä½¿å¯¹è±¡å¯JSONåºåˆ—åŒ–"""
        if isinstance(obj, dict):
            return {k: self._make_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_json_serializable(item) for item in obj]
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.bool_):
            return bool(obj)
        elif isinstance(obj, bool):
            return obj
        else:
            return obj
    
    def _generate_detection_report(self, results, timestamp):
        """ç”Ÿæˆæ£€æµ‹æŠ¥å‘Š"""
        report_path = os.path.join(self.output_dir, f"detection_report_{timestamp}.md")
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# è¿ç« å»ºç­‘æ£€æµ‹æŠ¥å‘Š\n\n")
            f.write(f"**æ£€æµ‹æ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}\n\n")
            
            # åŸºæœ¬ä¿¡æ¯
            analysis_info = results['analysis_info']
            f.write("## ğŸ“‹ åŸºæœ¬ä¿¡æ¯\n\n")
            f.write(f"- **ç¬¬ä¸€æœŸå›¾åƒ**: {os.path.basename(analysis_info['image1_path'])}\n")
            f.write(f"- **ç¬¬äºŒæœŸå›¾åƒ**: {os.path.basename(analysis_info['image2_path'])}\n")
            f.write(f"- **å›¾åƒå°ºå¯¸**: {analysis_info['image1_size'][:2]}\n\n")
            
            # æ£€æµ‹ç»“æœæ‘˜è¦
            pixel_comparison = results['pixel_comparison']
            f.write("## ğŸ¯ æ£€æµ‹ç»“æœæ‘˜è¦\n\n")
            f.write(f"- **ç–‘ä¼¼è¿ç« å»ºç­‘**: {pixel_comparison['suspicious_count']} ä¸ª\n")
            f.write(f"- **æ€»åŒºåŸŸå¯¹æ¯”**: {pixel_comparison['total_regions_compared']} å¯¹\n")
            f.write(f"- **æ–°å»ºç­‘æ£€æµ‹**: {pixel_comparison['new_buildings_detected']} ä¸ª\n\n")
            
            # è¯¦ç»†åˆ†æ
            if pixel_comparison['suspicious_regions']:
                f.write("## ğŸš¨ ç–‘ä¼¼åŒºåŸŸè¯¦æƒ…\n\n")
                for i, region in enumerate(pixel_comparison['suspicious_regions'], 1):
                    f.write(f"### ç–‘ä¼¼åŒºåŸŸ {i}\n")
                    f.write(f"- **åŒºåŸŸID**: {region['region_id']}\n")
                    f.write(f"- **ä½ç½®**: ({region['bbox'][0]}, {region['bbox'][1]})\n")
                    f.write(f"- **å°ºå¯¸**: {region['bbox'][2]}Ã—{region['bbox'][3]} åƒç´ \n")
                    f.write(f"- **å¹³å‡å·®å¼‚**: {region['mean_diff']:.2f}\n")
                    f.write(f"- **æœ€å¤§å·®å¼‚**: {region['max_diff']:.2f}\n")
                    f.write(f"- **è¶…é˜ˆå€¼æ¯”ä¾‹**: {region['threshold_ratio']:.2%}\n")
                    f.write(f"- **æ˜¯å¦æ–°å»ºç­‘**: {'æ˜¯' if region.get('is_new_building', False) else 'å¦'}\n\n")
            
            # æŠ€æœ¯å‚æ•°
            sys_params = results['system_parameters']
            f.write("## âš™ï¸ æŠ€æœ¯å‚æ•°\n\n")
            f.write(f"- **æœ€å°å»ºç­‘å°ºå¯¸**: {sys_params['min_building_size']}\n")
            f.write(f"- **åƒç´ å·®é˜ˆå€¼**: {sys_params['pixel_diff_threshold']}\n")
            f.write(f"- **è½®å»“é¢ç§¯é˜ˆå€¼**: {sys_params['contour_area_threshold']}\n")
            f.write(f"- **å…‰ç…§èŒƒå›´**: {sys_params['illumination_range']} lux\n\n")
            
            # å¤„ç†è¿‡ç¨‹
            f.write("## ğŸ”¬ å¤„ç†è¿‡ç¨‹\n\n")
            f.write("1. **é€è§†å˜æ¢æ ¡æ­£**: ä½¿ç”¨SIFTç‰¹å¾ç‚¹åŒ¹é…å’ŒRANSACç®—æ³•\n")
            f.write("2. **å›¾åƒå½’ä¸€åŒ–**: CLAHEå…‰ç…§å‡è¡¡åŒ–å’Œç›´æ–¹å›¾å‡è¡¡åŒ–\n")
            f.write("3. **å»ºç­‘ç‰©æ£€æµ‹**: Cannyè¾¹ç¼˜æ£€æµ‹ + è½®å»“ç­›é€‰\n")
            f.write("4. **åŒºåŸŸåŒ¹é…**: åŸºäºä½ç½®è·ç¦»çš„åŒºåŸŸå¯¹åº”\n")
            f.write("5. **åƒç´ å·®æ¯”è¾ƒ**: é€åƒç´ å·®å€¼è®¡ç®—å’Œé˜ˆå€¼åˆ†æ\n\n")
            
            f.write("---\n")
            f.write("*æœ¬æŠ¥å‘Šç”±è¿ç« å»ºç­‘æ£€æµ‹ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*")
        
        print(f"æ£€æµ‹æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")


def analyze_false_positive_regions(img1_path, img2_path, region_coords=None):
    """
    åˆ†æå¯èƒ½çš„å‡é˜³æ€§åŒºåŸŸ
    
    Args:
        img1_path (str): ç¬¬ä¸€æœŸå›¾åƒè·¯å¾„
        img2_path (str): ç¬¬äºŒæœŸå›¾åƒè·¯å¾„
        region_coords (tuple): å¯é€‰çš„åŒºåŸŸåæ ‡ (x, y, w, h)
    """
    detector = IllegalBuildingDetector()
    
    print("=== å‡é˜³æ€§åŒºåŸŸåˆ†æ ===")
    
    # åŠ è½½å›¾åƒ
    img1 = detector.load_image_with_chinese_path(img1_path)
    img2 = detector.load_image_with_chinese_path(img2_path)
    
    if img1 is None or img2 is None:
        return
    
    # é€è§†å˜æ¢æ ¡æ­£
    base_img, aligned_img, transform_matrix, transform_info = detector.perspective_correction(img1, img2)
    
    # åˆ†åˆ«åˆ†æåŸå§‹å›¾åƒå’Œå¤„ç†åå›¾åƒçš„å·®å¼‚
    print("\n1. åŸå§‹å›¾åƒå·®å¼‚åˆ†æ...")
    gray1_orig = cv2.cvtColor(base_img, cv2.COLOR_BGR2GRAY)
    gray2_orig = cv2.cvtColor(aligned_img, cv2.COLOR_BGR2GRAY)
    diff_orig = cv2.absdiff(gray1_orig, gray2_orig)
    
    print("\n2. å¤„ç†åå›¾åƒå·®å¼‚åˆ†æ...")
    normalized_img1, lighting_info1 = detector.adaptive_normalize_image(base_img)
    normalized_img2, lighting_info2 = detector.adaptive_normalize_image(aligned_img)
    gray1_proc = cv2.cvtColor(normalized_img1, cv2.COLOR_BGR2GRAY)
    gray2_proc = cv2.cvtColor(normalized_img2, cv2.COLOR_BGR2GRAY)
    diff_proc = cv2.absdiff(gray1_proc, gray2_proc)
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # åˆ›å»ºå¯¹æ¯”åˆ†æå›¾
    fig, axes = plt.subplots(3, 4, figsize=(20, 15))
    fig.suptitle('å‡é˜³æ€§åŒºåŸŸè¯Šæ–­åˆ†æ', fontsize=16, fontweight='bold')
    
    # ç¬¬ä¸€è¡Œï¼šåŸå§‹å›¾åƒå¯¹æ¯”
    axes[0, 0].imshow(cv2.cvtColor(base_img, cv2.COLOR_BGR2RGB))
    axes[0, 0].set_title('ç¬¬ä¸€æœŸå›¾åƒï¼ˆæ ¡æ­£åï¼‰', fontsize=12)
    axes[0, 0].axis('off')
    
    axes[0, 1].imshow(cv2.cvtColor(aligned_img, cv2.COLOR_BGR2RGB))
    axes[0, 1].set_title('ç¬¬äºŒæœŸå›¾åƒï¼ˆæ ¡æ­£åï¼‰', fontsize=12)
    axes[0, 1].axis('off')
    
    axes[0, 2].imshow(diff_orig, cmap='hot', vmin=0, vmax=255)
    axes[0, 2].set_title(f'åŸå§‹å·®å¼‚å›¾\nå¹³å‡å·®å¼‚: {np.mean(diff_orig):.2f}', fontsize=11)
    axes[0, 2].axis('off')
    
    # åŸå§‹å·®å¼‚ç›´æ–¹å›¾
    axes[0, 3].hist(diff_orig.flatten(), bins=50, alpha=0.7, color='blue', label='åŸå§‹å·®å¼‚')
    axes[0, 3].axvline(x=detector.pixel_diff_threshold, color='red', linestyle='--', label='é˜ˆå€¼')
    axes[0, 3].set_title('åŸå§‹å·®å¼‚åˆ†å¸ƒ', fontsize=11)
    axes[0, 3].legend()
    
    # ç¬¬äºŒè¡Œï¼šå¤„ç†åå›¾åƒå¯¹æ¯”
    axes[1, 0].imshow(cv2.cvtColor(normalized_img1, cv2.COLOR_BGR2RGB))
    axes[1, 0].set_title(f'ç¬¬ä¸€æœŸï¼ˆå¤„ç†åï¼‰\n{lighting_info1["lighting_type"]}', fontsize=11)
    axes[1, 0].axis('off')
    
    axes[1, 1].imshow(cv2.cvtColor(normalized_img2, cv2.COLOR_BGR2RGB))
    axes[1, 1].set_title(f'ç¬¬äºŒæœŸï¼ˆå¤„ç†åï¼‰\n{lighting_info2["lighting_type"]}', fontsize=11)
    axes[1, 1].axis('off')
    
    axes[1, 2].imshow(diff_proc, cmap='hot', vmin=0, vmax=255)
    axes[1, 2].set_title(f'å¤„ç†åå·®å¼‚å›¾\nå¹³å‡å·®å¼‚: {np.mean(diff_proc):.2f}', fontsize=11)
    axes[1, 2].axis('off')
    
    # å¤„ç†åå·®å¼‚ç›´æ–¹å›¾
    axes[1, 3].hist(diff_proc.flatten(), bins=50, alpha=0.7, color='green', label='å¤„ç†åå·®å¼‚')
    axes[1, 3].axvline(x=detector.pixel_diff_threshold, color='red', linestyle='--', label='é˜ˆå€¼')
    axes[1, 3].set_title('å¤„ç†åå·®å¼‚åˆ†å¸ƒ', fontsize=11)
    axes[1, 3].legend()
    
    # ç¬¬ä¸‰è¡Œï¼šå·®å¼‚å¯¹æ¯”å’Œåˆ†æ
    diff_comparison = diff_proc.astype(np.float32) - diff_orig.astype(np.float32)
    axes[2, 0].imshow(diff_comparison, cmap='RdBu', vmin=-100, vmax=100)
    axes[2, 0].set_title('å¤„ç†å¢åŠ çš„å·®å¼‚\n(è“è‰²=å‡å°‘, çº¢è‰²=å¢åŠ )', fontsize=11)
    axes[2, 0].axis('off')
    
    # é˜ˆå€¼æ©ç å¯¹æ¯”
    mask_orig = diff_orig > detector.pixel_diff_threshold
    mask_proc = diff_proc > detector.pixel_diff_threshold
    mask_diff = mask_proc.astype(np.uint8) - mask_orig.astype(np.uint8)
    
    axes[2, 1].imshow(mask_diff, cmap='RdBu', vmin=-1, vmax=1)
    axes[2, 1].set_title('é˜ˆå€¼æ©ç å˜åŒ–\n(çº¢è‰²=æ–°å¢å‡é˜³æ€§)', fontsize=11)
    axes[2, 1].axis('off')
    
    # ç»Ÿè®¡åˆ†æ
    orig_above_threshold = np.sum(mask_orig)
    proc_above_threshold = np.sum(mask_proc)
    false_positives = np.sum(mask_diff > 0)
    
    stats_text = f"""å·®å¼‚ç»Ÿè®¡åˆ†æ:

åŸå§‹è¶…é˜ˆå€¼åƒç´ : {orig_above_threshold:,}
å¤„ç†åè¶…é˜ˆå€¼åƒç´ : {proc_above_threshold:,}
æ–°å¢å‡é˜³æ€§åƒç´ : {false_positives:,}

æ¯”ä¾‹å˜åŒ–:
åŸå§‹: {orig_above_threshold/diff_orig.size*100:.2f}%
å¤„ç†å: {proc_above_threshold/diff_proc.size*100:.2f}%

å¤„ç†å½±å“:
{'å¢åŠ äº†å‡é˜³æ€§' if false_positives > orig_above_threshold*0.1 else 'å½±å“è¾ƒå°'}
"""
    
    axes[2, 2].text(0.05, 0.95, stats_text, ha='left', va='top', transform=axes[2, 2].transAxes,
                    fontsize=10, bbox=dict(boxstyle='round,pad=0.3', facecolor='lightblue', alpha=0.8))
    axes[2, 2].set_xlim(0, 1)
    axes[2, 2].set_ylim(0, 1)
    axes[2, 2].axis('off')
    axes[2, 2].set_title('ç»Ÿè®¡åˆ†æ', fontsize=12)
    
    # å»ºè®®ä¼˜åŒ–æ–¹æ¡ˆ
    suggestions_text = f"""ä¼˜åŒ–å»ºè®®:

1. è°ƒæ•´é˜ˆå€¼:
   å»ºè®®é˜ˆå€¼: {int(np.mean(diff_orig) + 2*np.std(diff_orig))}
   (å½“å‰: {detector.pixel_diff_threshold})

2. å¤„ç†å‚æ•°ä¼˜åŒ–:
   {'å‡å°‘CLAHEå¼ºåº¦' if np.mean(diff_proc) > np.mean(diff_orig)*1.2 else 'å‚æ•°åˆé€‚'}

3. é…å‡†ç²¾åº¦:
   å†…ç‚¹ç‡: {transform_info.get('inlier_ratio', 0):.1%}
   {'å»ºè®®æé«˜é…å‡†ç²¾åº¦' if transform_info.get('inlier_ratio', 0) < 0.6 else 'é…å‡†ç²¾åº¦è‰¯å¥½'}

4. åå¤„ç†å»ºè®®:
   åº”ç”¨å½¢æ€å­¦æ»¤æ³¢
   åŒºåŸŸè¿é€šæ€§åˆ†æ
"""
    
    axes[2, 3].text(0.05, 0.95, suggestions_text, ha='left', va='top', transform=axes[2, 3].transAxes,
                    fontsize=10, bbox=dict(boxstyle='round,pad=0.3', facecolor='lightgreen', alpha=0.8))
    axes[2, 3].set_xlim(0, 1)
    axes[2, 3].set_ylim(0, 1)
    axes[2, 3].axis('off')
    axes[2, 3].set_title('ä¼˜åŒ–å»ºè®®', fontsize=12)
    
    plt.tight_layout()
    plt.show()
    
    # è¾“å‡ºè¯¦ç»†åˆ†æ
    print(f"\nğŸ“Š è¯¦ç»†åˆ†æç»“æœ:")
    print(f"   åŸå§‹å¹³å‡å·®å¼‚: {np.mean(diff_orig):.2f}")
    print(f"   å¤„ç†åå¹³å‡å·®å¼‚: {np.mean(diff_proc):.2f}")
    print(f"   å·®å¼‚å¢å¹…: {(np.mean(diff_proc)/np.mean(diff_orig)-1)*100:.1f}%")
    print(f"   æ–°å¢å‡é˜³æ€§åƒç´ : {false_positives:,} ({false_positives/diff_orig.size*100:.3f}%)")
    
    return {
        'original_diff': diff_orig,
        'processed_diff': diff_proc,
        'false_positives': false_positives,
        'suggestions': {
            'recommended_threshold': int(np.mean(diff_orig) + 2*np.std(diff_orig)),
            'current_threshold': detector.pixel_diff_threshold
        }
    }


def show_processed_comparison(img1_path, img2_path):
    """
    æ˜¾ç¤ºå¤„ç†åçš„ä¸¤å¼ å›¾åƒå¯¹æ¯”
    
    Args:
        img1_path (str): ç¬¬ä¸€æœŸå›¾åƒè·¯å¾„
        img2_path (str): ç¬¬äºŒæœŸå›¾åƒè·¯å¾„
    """
    # åˆ›å»ºæ£€æµ‹å™¨å®ä¾‹
    detector = IllegalBuildingDetector()
    
    print("åŠ è½½å›¾åƒ...")
    # åŠ è½½å›¾åƒ
    img1 = detector.load_image_with_chinese_path(img1_path)
    img2 = detector.load_image_with_chinese_path(img2_path)
    
    if img1 is None or img2 is None:
        print("å›¾åƒåŠ è½½å¤±è´¥")
        return
    
    print("æ‰§è¡Œé€è§†å˜æ¢æ ¡æ­£...")
    # é€è§†å˜æ¢æ ¡æ­£
    base_img, aligned_img, transform_matrix, transform_info = detector.perspective_correction(img1, img2)
    
    print("æ‰§è¡Œè‡ªé€‚åº”å›¾åƒå½’ä¸€åŒ–...")
    # è‡ªé€‚åº”å›¾åƒå½’ä¸€åŒ–
    print("  åˆ†æç¬¬ä¸€æœŸå›¾åƒå…‰ç…§...")
    normalized_img1, lighting_info1 = detector.adaptive_normalize_image(base_img)
    print("  åˆ†æç¬¬äºŒæœŸå›¾åƒå…‰ç…§...")
    normalized_img2, lighting_info2 = detector.adaptive_normalize_image(aligned_img)
    
    print("è®¡ç®—å·®å¼‚å›¾...")
    # è®¡ç®—å·®å¼‚å›¾
    gray1 = cv2.cvtColor(normalized_img1, cv2.COLOR_BGR2GRAY)
    gray2 = cv2.cvtColor(normalized_img2, cv2.COLOR_BGR2GRAY)
    diff_img = cv2.absdiff(gray1, gray2)
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # åˆ›å»ºæ˜¾ç¤ºçª—å£
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('å›¾åƒå¤„ç†å¯¹æ¯”æ˜¾ç¤º', fontsize=16, fontweight='bold')
    
    # ç¬¬ä¸€è¡Œï¼šåŸå§‹å›¾åƒ
    axes[0, 0].imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))
    axes[0, 0].set_title('ç¬¬ä¸€æœŸå›¾åƒï¼ˆåŸå§‹ï¼‰', fontsize=12)
    axes[0, 0].axis('off')
    
    axes[0, 1].imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))
    axes[0, 1].set_title('ç¬¬äºŒæœŸå›¾åƒï¼ˆåŸå§‹ï¼‰', fontsize=12)
    axes[0, 1].axis('off')
    
    axes[0, 2].imshow(cv2.cvtColor(aligned_img, cv2.COLOR_BGR2RGB))
    axes[0, 2].set_title(f'é€è§†æ ¡æ­£å\nå†…ç‚¹ç‡: {transform_info.get("inlier_ratio", 0):.2%}', fontsize=12)
    axes[0, 2].axis('off')
    
    # ç¬¬äºŒè¡Œï¼šå¤„ç†åå›¾åƒå’Œå·®å¼‚å›¾
    axes[1, 0].imshow(cv2.cvtColor(normalized_img1, cv2.COLOR_BGR2RGB))
    axes[1, 0].set_title('ç¬¬ä¸€æœŸå›¾åƒï¼ˆå¤„ç†åï¼‰', fontsize=12)
    axes[1, 0].axis('off')
    
    axes[1, 1].imshow(cv2.cvtColor(normalized_img2, cv2.COLOR_BGR2RGB))
    axes[1, 1].set_title('ç¬¬äºŒæœŸå›¾åƒï¼ˆå¤„ç†åï¼‰', fontsize=12)
    axes[1, 1].axis('off')
    
    # å·®å¼‚å›¾
    im = axes[1, 2].imshow(diff_img, cmap='hot')
    axes[1, 2].set_title(f'å·®å¼‚å›¾\né˜ˆå€¼: {detector.pixel_diff_threshold}', fontsize=12)
    axes[1, 2].axis('off')
    plt.colorbar(im, ax=axes[1, 2], fraction=0.046, pad=0.04)
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    total_pixels = diff_img.size
    significant_diff_pixels = np.sum(diff_img > detector.pixel_diff_threshold)
    diff_percentage = (significant_diff_pixels / total_pixels) * 100
    mean_diff = np.mean(diff_img)
    
    print(f"\nğŸ“Š å¤„ç†ç»“æœç»Ÿè®¡:")
    print(f"   é€è§†å˜æ¢å†…ç‚¹ç‡: {transform_info.get('inlier_ratio', 0):.2%}")
    print(f"   å¹³å‡åƒç´ å·®å¼‚: {mean_diff:.2f}")
    print(f"   æ˜¾è‘—å·®å¼‚æ¯”ä¾‹: {diff_percentage:.2f}%")
    print(f"   åŒ¹é…ç‰¹å¾ç‚¹: {transform_info.get('matches', 0)} ä¸ª")
    
    print(f"\nğŸ”† å…‰ç…§åˆ†æç»“æœ:")
    print(f"   ç¬¬ä¸€æœŸå›¾åƒ: {lighting_info1['lighting_type']} (äº®åº¦: {lighting_info1['mean_brightness']:.1f})")
    print(f"   ç¬¬äºŒæœŸå›¾åƒ: {lighting_info2['lighting_type']} (äº®åº¦: {lighting_info2['mean_brightness']:.1f})")
    print(f"   å…‰ç…§ä¸€è‡´æ€§: {'è‰¯å¥½' if abs(lighting_info1['mean_brightness'] - lighting_info2['mean_brightness']) < 30 else 'éœ€è¦æ³¨æ„'}")
    
    plt.tight_layout()
    plt.show()


def main():
    """ä¸»å‡½æ•°"""
    # æµ‹è¯•å›¾åƒè·¯å¾„
    img1_path = r'C:\Users\admin\Desktop\ä¸¤æœŸæ¯”å¯¹\ä¸¤æœŸæ¯”å¯¹\Snipaste_2025-09-08_15-39-45.png'  # ç¬¬ä¸€æœŸå›¾åƒ
    img2_path = r'C:\Users\admin\Desktop\ä¸¤æœŸæ¯”å¯¹\ä¸¤æœŸæ¯”å¯¹\Snipaste_2025-09-08_15-39-16.png'  # ç¬¬äºŒæœŸå›¾åƒ
    
    try:
        # åˆ›å»ºè¿ç« å»ºç­‘æ£€æµ‹ç³»ç»Ÿ
        detector = IllegalBuildingDetector()
        
        # æ‰§è¡Œæ£€æµ‹
        result = detector.process_illegal_building_detection(img1_path, img2_path)
        
        if result:
            # ä¿å­˜ç»“æœ
            detector.save_detection_results(result)
            
            print(f"\nğŸ“Š æ£€æµ‹ç»“æœç»Ÿè®¡:")
            print(f"   ç¬¬ä¸€æœŸå»ºç­‘ç‰©: {result['building_detection']['period1_buildings']} ä¸ª")
            print(f"   ç¬¬äºŒæœŸå»ºç­‘ç‰©: {result['building_detection']['period2_buildings']} ä¸ª")
            print(f"   ç–‘ä¼¼è¿ç« å»ºç­‘: {result['pixel_comparison']['suspicious_count']} ä¸ª")
            print(f"   æ–°å»ºç­‘ç‰©: {result['pixel_comparison']['new_buildings_detected']} ä¸ª")
            print(f"ğŸ“ å¯è§†åŒ–å›¾åƒ: {os.path.basename(result['visualization_path'])}")
        else:
            print("âŒ æ£€æµ‹å¤±è´¥")
            
    except Exception as e:
        print(f"âŒ è¿ç« å»ºç­‘æ£€æµ‹æ—¶å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 

# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================
# å¦‚æœåªæƒ³æ˜¾ç¤ºå¤„ç†åçš„å›¾åƒå¯¹æ¯”ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç ï¼š
# 
# if __name__ == "__main__":
#     # æ›¿æ¢ä¸ºæ‚¨çš„å›¾åƒè·¯å¾„
#     img1_path = r'C:\Users\admin\Desktop\ä¸¤æœŸæ¯”å¯¹\ä¸¤æœŸæ¯”å¯¹\image1.png'
#     img2_path = r'C:\Users\admin\Desktop\ä¸¤æœŸæ¯”å¯¹\ä¸¤æœŸæ¯”å¯¹\image2.png'
#     
#     # æ˜¾ç¤ºå¤„ç†å¯¹æ¯”ï¼ˆä¸ä¿å­˜æ–‡ä»¶ï¼‰
#     show_processed_comparison(img1_path, img2_path) 

def show_alignment_check(img1_path, img2_path):
    """
    æ˜¾ç¤ºé€è§†æ ¡æ­£åçš„é‡å æ£€æŸ¥å›¾åƒ
    ç”¨äºæ£€æµ‹é…å‡†åç§»å’Œæ ¡æ­£è´¨é‡
    
    Args:
        img1_path (str): ç¬¬ä¸€æœŸå›¾åƒè·¯å¾„
        img2_path (str): ç¬¬äºŒæœŸå›¾åƒè·¯å¾„
    """
    detector = IllegalBuildingDetector()
    
    print("=== é€è§†æ ¡æ­£é‡å æ£€æŸ¥ ===")
    
    # åŠ è½½å›¾åƒ
    img1 = detector.load_image_with_chinese_path(img1_path)
    img2 = detector.load_image_with_chinese_path(img2_path)
    
    if img1 is None or img2 is None:
        print("å›¾åƒåŠ è½½å¤±è´¥")
        return
    
    print(f"åŸå§‹å›¾åƒå°ºå¯¸: {img1.shape} vs {img2.shape}")
    
    # é€è§†å˜æ¢æ ¡æ­£
    base_img, aligned_img, transform_matrix, transform_info = detector.perspective_correction(img1, img2)
    
    print(f"æ ¡æ­£åå›¾åƒå°ºå¯¸: {base_img.shape} vs {aligned_img.shape}")
    print(f"é€è§†å˜æ¢å†…ç‚¹ç‡: {transform_info.get('inlier_ratio', 0):.2%}")
    print(f"åŒ¹é…ç‰¹å¾ç‚¹æ•°: {transform_info.get('matches', 0)}")
    
    # åˆ›å»ºä¸åŒçš„é‡å å¯è§†åŒ–
    h, w = base_img.shape[:2]
    
    # 1. æ£‹ç›˜æ ¼é‡å  - ç”¨äºæ£€æµ‹é…å‡†ç²¾åº¦
    def create_checkerboard_overlay(img1, img2, block_size=50):
        overlay = img1.copy()
        for i in range(0, h, block_size):
            for j in range(0, w, block_size):
                if (i // block_size + j // block_size) % 2 == 1:
                    i_end = min(i + block_size, h)
                    j_end = min(j + block_size, w)
                    overlay[i:i_end, j:j_end] = img2[i:i_end, j:j_end]
        return overlay
    
    # 2. åŠé€æ˜é‡å  - ç”¨äºæ£€æµ‹æ•´ä½“åç§»
    def create_alpha_blend(img1, img2, alpha=0.5):
        return cv2.addWeighted(img1, alpha, img2, 1-alpha, 0)
    
    # 3. çº¢ç»¿é‡å  - ç”¨äºæ£€æµ‹ç»†å¾®åç§»
    def create_red_green_overlay(img1, img2):
        overlay = np.zeros_like(img1)
        gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
        gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
        overlay[:, :, 1] = gray1  # ç¬¬ä¸€æœŸå›¾åƒ -> ç»¿è‰²é€šé“
        overlay[:, :, 2] = gray2  # ç¬¬äºŒæœŸå›¾åƒ -> çº¢è‰²é€šé“
        return overlay
    
    # 4. è¾¹ç¼˜é‡å  - ç”¨äºæ£€æµ‹ç»“æ„åç§»
    def create_edge_overlay(img1, img2):
        gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
        gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
        
        # æå–è¾¹ç¼˜
        edges1 = cv2.Canny(gray1, 50, 150)
        edges2 = cv2.Canny(gray2, 50, 150)
        
        # åˆ›å»ºå½©è‰²è¾¹ç¼˜é‡å 
        overlay = np.zeros((h, w, 3), dtype=np.uint8)
        overlay[:, :, 1] = edges1  # ç¬¬ä¸€æœŸè¾¹ç¼˜ -> ç»¿è‰²
        overlay[:, :, 2] = edges2  # ç¬¬äºŒæœŸè¾¹ç¼˜ -> çº¢è‰²
        
        # é‡å åŒºåŸŸæ˜¾ç¤ºä¸ºé»„è‰²
        overlap = np.logical_and(edges1 > 0, edges2 > 0)
        overlay[overlap] = [0, 255, 255]
        
        return overlay
    
    # åˆ›å»ºå„ç§é‡å å›¾åƒ
    checkerboard = create_checkerboard_overlay(base_img, aligned_img)
    alpha_blend = create_alpha_blend(base_img, aligned_img)
    red_green = create_red_green_overlay(base_img, aligned_img)
    edge_overlay = create_edge_overlay(base_img, aligned_img)
    
    # è®¡ç®—é…å‡†è´¨é‡æŒ‡æ ‡
    gray1 = cv2.cvtColor(base_img, cv2.COLOR_BGR2GRAY)
    gray2 = cv2.cvtColor(aligned_img, cv2.COLOR_BGR2GRAY)
    
    # ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•° (SSIM)
    from skimage.metrics import structural_similarity as ssim
    ssim_score = ssim(gray1, gray2)
    
    # å½’ä¸€åŒ–äº’ç›¸å…³ (NCC)
    def normalized_cross_correlation(img1, img2):
        img1_norm = (img1 - np.mean(img1)) / np.std(img1)
        img2_norm = (img2 - np.mean(img2)) / np.std(img2)
        return np.mean(img1_norm * img2_norm)
    
    ncc_score = normalized_cross_correlation(gray1, gray2)
    
    # å‡æ–¹è¯¯å·® (MSE)
    mse_score = np.mean((gray1.astype(np.float32) - gray2.astype(np.float32)) ** 2)
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # åˆ›å»ºæ˜¾ç¤ºçª—å£
    fig, axes = plt.subplots(3, 3, figsize=(18, 18))
    fig.suptitle('é€è§†æ ¡æ­£é‡å æ£€æŸ¥åˆ†æ', fontsize=16, fontweight='bold')
    
    # ç¬¬ä¸€è¡Œï¼šåŸå§‹å›¾åƒå’Œæ ¡æ­£åå›¾åƒ
    axes[0, 0].imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))
    axes[0, 0].set_title('ç¬¬ä¸€æœŸå›¾åƒï¼ˆåŸºå‡†ï¼‰', fontsize=12)
    axes[0, 0].axis('off')
    
    axes[0, 1].imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))
    axes[0, 1].set_title('ç¬¬äºŒæœŸå›¾åƒï¼ˆåŸå§‹ï¼‰', fontsize=12)
    axes[0, 1].axis('off')
    
    axes[0, 2].imshow(cv2.cvtColor(aligned_img, cv2.COLOR_BGR2RGB))
    axes[0, 2].set_title(f'ç¬¬äºŒæœŸå›¾åƒï¼ˆæ ¡æ­£åï¼‰\nå†…ç‚¹ç‡: {transform_info.get("inlier_ratio", 0):.2%}', fontsize=12)
    axes[0, 2].axis('off')
    
    # ç¬¬äºŒè¡Œï¼šé‡å æ£€æŸ¥
    axes[1, 0].imshow(cv2.cvtColor(checkerboard, cv2.COLOR_BGR2RGB))
    axes[1, 0].set_title('æ£‹ç›˜æ ¼é‡å \nï¼ˆæ£€æµ‹é…å‡†ç²¾åº¦ï¼‰', fontsize=12)
    axes[1, 0].axis('off')
    
    axes[1, 1].imshow(cv2.cvtColor(alpha_blend, cv2.COLOR_BGR2RGB))
    axes[1, 1].set_title('åŠé€æ˜é‡å \nï¼ˆæ£€æµ‹æ•´ä½“åç§»ï¼‰', fontsize=12)
    axes[1, 1].axis('off')
    
    axes[1, 2].imshow(cv2.cvtColor(red_green, cv2.COLOR_BGR2RGB))
    axes[1, 2].set_title('çº¢ç»¿é‡å \nï¼ˆçº¢=æœŸäºŒï¼Œç»¿=æœŸä¸€ï¼‰', fontsize=12)
    axes[1, 2].axis('off')
    
    # ç¬¬ä¸‰è¡Œï¼šè¾¹ç¼˜åˆ†æå’Œè´¨é‡æŒ‡æ ‡
    axes[2, 0].imshow(cv2.cvtColor(edge_overlay, cv2.COLOR_BGR2RGB))
    axes[2, 0].set_title('è¾¹ç¼˜é‡å \nï¼ˆé»„=é‡åˆï¼Œçº¢ç»¿=åç§»ï¼‰', fontsize=12)
    axes[2, 0].axis('off')
    
    # æ˜¾ç¤ºé…å‡†è´¨é‡æŒ‡æ ‡
    quality_text = f"""é…å‡†è´¨é‡è¯„ä¼°:

SSIM (ç»“æ„ç›¸ä¼¼æ€§):
{ssim_score:.4f}
(1.0 = å®Œå…¨ç›¸ä¼¼)

NCC (å½’ä¸€åŒ–äº’ç›¸å…³):
{ncc_score:.4f}
(1.0 = å®Œå…¨ç›¸å…³)

MSE (å‡æ–¹è¯¯å·®):
{mse_score:.2f}
(0 = å®Œå…¨åŒ¹é…)

æ€»ä½“è¯„ä¼°:
{'ä¼˜ç§€' if ssim_score > 0.9 else 'è‰¯å¥½' if ssim_score > 0.8 else 'ä¸€èˆ¬' if ssim_score > 0.7 else 'è¾ƒå·®'}
"""
    
    color = 'lightgreen' if ssim_score > 0.8 else 'lightyellow' if ssim_score > 0.7 else 'lightcoral'
    axes[2, 1].text(0.05, 0.95, quality_text, ha='left', va='top', transform=axes[2, 1].transAxes,
                    fontsize=11, bbox=dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.8))
    axes[2, 1].set_xlim(0, 1)
    axes[2, 1].set_ylim(0, 1)
    axes[2, 1].axis('off')
    axes[2, 1].set_title('è´¨é‡æŒ‡æ ‡', fontsize=12)
    
    # æ˜¾ç¤ºåç§»è¯Šæ–­å»ºè®®
    diagnostic_text = f"""åç§»è¯Šæ–­å»ºè®®:

1. ç‰¹å¾ç‚¹åŒ¹é…:
   åŒ¹é…ç‚¹æ•°: {transform_info.get('matches', 0)}
   å†…ç‚¹ç‡: {transform_info.get('inlier_ratio', 0):.1%}
   {'âœ… åŒ¹é…è‰¯å¥½' if transform_info.get('inlier_ratio', 0) > 0.6 else 'âš ï¸ åŒ¹é…ä¸ä½³'}

2. é…å‡†ç²¾åº¦:
   SSIMè¯„åˆ†: {ssim_score:.3f}
   {'âœ… é…å‡†ç²¾ç¡®' if ssim_score > 0.85 else 'âš ï¸ å­˜åœ¨åç§»'}

3. æ”¹è¿›å»ºè®®:
   {'å¢åŠ ç‰¹å¾ç‚¹æ•°é‡' if transform_info.get('matches', 0) < 100 else ''}
   {'æé«˜å›¾åƒè´¨é‡' if ssim_score < 0.8 else ''}
   {'æ£€æŸ¥å›¾åƒå†…å®¹å·®å¼‚' if ncc_score < 0.7 else ''}

4. åç§»å½±å“:
   {'å¯èƒ½å½±å“å·®å¼‚æ£€æµ‹å‡†ç¡®æ€§' if ssim_score < 0.8 else 'å¯¹å·®å¼‚æ£€æµ‹å½±å“è¾ƒå°'}
"""
    
    axes[2, 2].text(0.05, 0.95, diagnostic_text, ha='left', va='top', transform=axes[2, 2].transAxes,
                    fontsize=10, bbox=dict(boxstyle='round,pad=0.3', facecolor='lightblue', alpha=0.8))
    axes[2, 2].set_xlim(0, 1)
    axes[2, 2].set_ylim(0, 1)
    axes[2, 2].axis('off')
    axes[2, 2].set_title('è¯Šæ–­å»ºè®®', fontsize=12)
    
    plt.tight_layout()
    plt.show()
    
    # è¾“å‡ºè¯¦ç»†åˆ†æç»“æœ
    print(f"\nğŸ“Š é…å‡†è´¨é‡åˆ†æ:")
    print(f"   SSIM (ç»“æ„ç›¸ä¼¼æ€§): {ssim_score:.4f}")
    print(f"   NCC (å½’ä¸€åŒ–äº’ç›¸å…³): {ncc_score:.4f}")
    print(f"   MSE (å‡æ–¹è¯¯å·®): {mse_score:.2f}")
    print(f"   é…å‡†è´¨é‡: {'ä¼˜ç§€' if ssim_score > 0.9 else 'è‰¯å¥½' if ssim_score > 0.8 else 'ä¸€èˆ¬' if ssim_score > 0.7 else 'è¾ƒå·®'}")
    
    if ssim_score < 0.8:
        print(f"\nâš ï¸  é…å‡†è´¨é‡è­¦å‘Š:")
        print(f"   SSIMè¯„åˆ† {ssim_score:.3f} ä½äº0.8ï¼Œå¯èƒ½å­˜åœ¨æ˜æ˜¾åç§»")
        print(f"   è¿™å¯èƒ½å¯¼è‡´å‡é˜³æ€§å·®å¼‚å¢åŠ ")
        print(f"   å»ºè®®æ£€æŸ¥ç‰¹å¾ç‚¹åˆ†å¸ƒå’Œå›¾åƒè´¨é‡")
    
    return {
        'ssim': ssim_score,
        'ncc': ncc_score,
        'mse': mse_score,
        'transform_info': transform_info,
        'alignment_quality': 'excellent' if ssim_score > 0.9 else 'good' if ssim_score > 0.8 else 'fair' if ssim_score > 0.7 else 'poor'
    }


def analyze_false_positive_regions(img1_path, img2_path, region_coords=None):
    """
    åˆ†æå¯èƒ½çš„å‡é˜³æ€§åŒºåŸŸ
    
    Args:
        img1_path (str): ç¬¬ä¸€æœŸå›¾åƒè·¯å¾„
        img2_path (str): ç¬¬äºŒæœŸå›¾åƒè·¯å¾„
        region_coords (tuple): å¯é€‰çš„åŒºåŸŸåæ ‡ (x, y, w, h)
    """
    detector = IllegalBuildingDetector()
    
    print("=== å‡é˜³æ€§åŒºåŸŸåˆ†æ ===")
    
    # åŠ è½½å›¾åƒ
    img1 = detector.load_image_with_chinese_path(img1_path)
    img2 = detector.load_image_with_chinese_path(img2_path)
    
    if img1 is None or img2 is None:
        return
    
    # é€è§†å˜æ¢æ ¡æ­£
    base_img, aligned_img, transform_matrix, transform_info = detector.perspective_correction(img1, img2)
    
    # åˆ†åˆ«åˆ†æåŸå§‹å›¾åƒå’Œå¤„ç†åå›¾åƒçš„å·®å¼‚
    print("\n1. åŸå§‹å›¾åƒå·®å¼‚åˆ†æ...")
    gray1_orig = cv2.cvtColor(base_img, cv2.COLOR_BGR2GRAY)
    gray2_orig = cv2.cvtColor(aligned_img, cv2.COLOR_BGR2GRAY)
    diff_orig = cv2.absdiff(gray1_orig, gray2_orig)
    
    print("\n2. å¤„ç†åå›¾åƒå·®å¼‚åˆ†æ...")
    normalized_img1, lighting_info1 = detector.adaptive_normalize_image(base_img)
    normalized_img2, lighting_info2 = detector.adaptive_normalize_image(aligned_img)
    gray1_proc = cv2.cvtColor(normalized_img1, cv2.COLOR_BGR2GRAY)
    gray2_proc = cv2.cvtColor(normalized_img2, cv2.COLOR_BGR2GRAY)
    diff_proc = cv2.absdiff(gray1_proc, gray2_proc)
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # åˆ›å»ºå¯¹æ¯”åˆ†æå›¾
    fig, axes = plt.subplots(3, 4, figsize=(20, 15))
    fig.suptitle('å‡é˜³æ€§åŒºåŸŸè¯Šæ–­åˆ†æ', fontsize=16, fontweight='bold')
    
    # ç¬¬ä¸€è¡Œï¼šåŸå§‹å›¾åƒå¯¹æ¯”
    axes[0, 0].imshow(cv2.cvtColor(base_img, cv2.COLOR_BGR2RGB))
    axes[0, 0].set_title('ç¬¬ä¸€æœŸå›¾åƒï¼ˆæ ¡æ­£åï¼‰', fontsize=12)
    axes[0, 0].axis('off')
    
    axes[0, 1].imshow(cv2.cvtColor(aligned_img, cv2.COLOR_BGR2RGB))
    axes[0, 1].set_title('ç¬¬äºŒæœŸå›¾åƒï¼ˆæ ¡æ­£åï¼‰', fontsize=12)
    axes[0, 1].axis('off')
    
    axes[0, 2].imshow(diff_orig, cmap='hot', vmin=0, vmax=255)
    axes[0, 2].set_title(f'åŸå§‹å·®å¼‚å›¾\nå¹³å‡å·®å¼‚: {np.mean(diff_orig):.2f}', fontsize=11)
    axes[0, 2].axis('off')
    
    # åŸå§‹å·®å¼‚ç›´æ–¹å›¾
    axes[0, 3].hist(diff_orig.flatten(), bins=50, alpha=0.7, color='blue', label='åŸå§‹å·®å¼‚')
    axes[0, 3].axvline(x=detector.pixel_diff_threshold, color='red', linestyle='--', label='é˜ˆå€¼')
    axes[0, 3].set_title('åŸå§‹å·®å¼‚åˆ†å¸ƒ', fontsize=11)
    axes[0, 3].legend()
    
    # ç¬¬äºŒè¡Œï¼šå¤„ç†åå›¾åƒå¯¹æ¯”
    axes[1, 0].imshow(cv2.cvtColor(normalized_img1, cv2.COLOR_BGR2RGB))
    axes[1, 0].set_title(f'ç¬¬ä¸€æœŸï¼ˆå¤„ç†åï¼‰\n{lighting_info1["lighting_type"]}', fontsize=11)
    axes[1, 0].axis('off')
    
    axes[1, 1].imshow(cv2.cvtColor(normalized_img2, cv2.COLOR_BGR2RGB))
    axes[1, 1].set_title(f'ç¬¬äºŒæœŸï¼ˆå¤„ç†åï¼‰\n{lighting_info2["lighting_type"]}', fontsize=11)
    axes[1, 1].axis('off')
    
    axes[1, 2].imshow(diff_proc, cmap='hot', vmin=0, vmax=255)
    axes[1, 2].set_title(f'å¤„ç†åå·®å¼‚å›¾\nå¹³å‡å·®å¼‚: {np.mean(diff_proc):.2f}', fontsize=11)
    axes[1, 2].axis('off')
    
    # å¤„ç†åå·®å¼‚ç›´æ–¹å›¾
    axes[1, 3].hist(diff_proc.flatten(), bins=50, alpha=0.7, color='green', label='å¤„ç†åå·®å¼‚')
    axes[1, 3].axvline(x=detector.pixel_diff_threshold, color='red', linestyle='--', label='é˜ˆå€¼')
    axes[1, 3].set_title('å¤„ç†åå·®å¼‚åˆ†å¸ƒ', fontsize=11)
    axes[1, 3].legend()
    
    # ç¬¬ä¸‰è¡Œï¼šå·®å¼‚å¯¹æ¯”å’Œåˆ†æ
    diff_comparison = diff_proc.astype(np.float32) - diff_orig.astype(np.float32)
    axes[2, 0].imshow(diff_comparison, cmap='RdBu', vmin=-100, vmax=100)
    axes[2, 0].set_title('å¤„ç†å¢åŠ çš„å·®å¼‚\n(è“è‰²=å‡å°‘, çº¢è‰²=å¢åŠ )', fontsize=11)
    axes[2, 0].axis('off')
    
    # é˜ˆå€¼æ©ç å¯¹æ¯”
    mask_orig = diff_orig > detector.pixel_diff_threshold
    mask_proc = diff_proc > detector.pixel_diff_threshold
    mask_diff = mask_proc.astype(np.uint8) - mask_orig.astype(np.uint8)
    
    axes[2, 1].imshow(mask_diff, cmap='RdBu', vmin=-1, vmax=1)
    axes[2, 1].set_title('é˜ˆå€¼æ©ç å˜åŒ–\n(çº¢è‰²=æ–°å¢å‡é˜³æ€§)', fontsize=11)
    axes[2, 1].axis('off')
    
    # ç»Ÿè®¡åˆ†æ
    orig_above_threshold = np.sum(mask_orig)
    proc_above_threshold = np.sum(mask_proc)
    false_positives = np.sum(mask_diff > 0)
    
    stats_text = f"""å·®å¼‚ç»Ÿè®¡åˆ†æ:

åŸå§‹è¶…é˜ˆå€¼åƒç´ : {orig_above_threshold:,}
å¤„ç†åè¶…é˜ˆå€¼åƒç´ : {proc_above_threshold:,}
æ–°å¢å‡é˜³æ€§åƒç´ : {false_positives:,}

æ¯”ä¾‹å˜åŒ–:
åŸå§‹: {orig_above_threshold/diff_orig.size*100:.2f}%
å¤„ç†å: {proc_above_threshold/diff_proc.size*100:.2f}%

å¤„ç†å½±å“:
{'å¢åŠ äº†å‡é˜³æ€§' if false_positives > orig_above_threshold*0.1 else 'å½±å“è¾ƒå°'}
"""
    
    axes[2, 2].text(0.05, 0.95, stats_text, ha='left', va='top', transform=axes[2, 2].transAxes,
                    fontsize=10, bbox=dict(boxstyle='round,pad=0.3', facecolor='lightblue', alpha=0.8))
    axes[2, 2].set_xlim(0, 1)
    axes[2, 2].set_ylim(0, 1)
    axes[2, 2].axis('off')
    axes[2, 2].set_title('ç»Ÿè®¡åˆ†æ', fontsize=12)
    
    # å»ºè®®ä¼˜åŒ–æ–¹æ¡ˆ
    suggestions_text = f"""ä¼˜åŒ–å»ºè®®:

1. è°ƒæ•´é˜ˆå€¼:
   å»ºè®®é˜ˆå€¼: {int(np.mean(diff_orig) + 2*np.std(diff_orig))}
   (å½“å‰: {detector.pixel_diff_threshold})

2. å¤„ç†å‚æ•°ä¼˜åŒ–:
   {'å‡å°‘CLAHEå¼ºåº¦' if np.mean(diff_proc) > np.mean(diff_orig)*1.2 else 'å‚æ•°åˆé€‚'}

3. é…å‡†ç²¾åº¦:
   å†…ç‚¹ç‡: {transform_info.get('inlier_ratio', 0):.1%}
   {'å»ºè®®æé«˜é…å‡†ç²¾åº¦' if transform_info.get('inlier_ratio', 0) < 0.6 else 'é…å‡†ç²¾åº¦è‰¯å¥½'}

4. åå¤„ç†å»ºè®®:
   åº”ç”¨å½¢æ€å­¦æ»¤æ³¢
   åŒºåŸŸè¿é€šæ€§åˆ†æ
"""
    
    axes[2, 3].text(0.05, 0.95, suggestions_text, ha='left', va='top', transform=axes[2, 3].transAxes,
                    fontsize=10, bbox=dict(boxstyle='round,pad=0.3', facecolor='lightgreen', alpha=0.8))
    axes[2, 3].set_xlim(0, 1)
    axes[2, 3].set_ylim(0, 1)
    axes[2, 3].axis('off')
    axes[2, 3].set_title('ä¼˜åŒ–å»ºè®®', fontsize=12)
    
    plt.tight_layout()
    plt.show()
    
    # è¾“å‡ºè¯¦ç»†åˆ†æ
    print(f"\nğŸ“Š è¯¦ç»†åˆ†æç»“æœ:")
    print(f"   åŸå§‹å¹³å‡å·®å¼‚: {np.mean(diff_orig):.2f}")
    print(f"   å¤„ç†åå¹³å‡å·®å¼‚: {np.mean(diff_proc):.2f}")
    print(f"   å·®å¼‚å¢å¹…: {(np.mean(diff_proc)/np.mean(diff_orig)-1)*100:.1f}%")
    print(f"   æ–°å¢å‡é˜³æ€§åƒç´ : {false_positives:,} ({false_positives/diff_orig.size*100:.3f}%)")
    
    return {
        'original_diff': diff_orig,
        'processed_diff': diff_proc,
        'false_positives': false_positives,
        'suggestions': {
            'recommended_threshold': int(np.mean(diff_orig) + 2*np.std(diff_orig)),
            'current_threshold': detector.pixel_diff_threshold
        }
    }


def elastic_registration_with_similarity_matching(img1_path, img2_path):
    """
    åŸºäºç›¸ä¼¼æ€§åŒ¹é…çš„å¼¹æ€§é…å‡†æ–¹æ³•
    å°†å¤§é¢ç§¯ç›¸ä¼¼çš„åŒºåŸŸè‡ªåŠ¨å¯¹é½ï¼Œè§£å†³é€è§†æ ¡æ­£æ— æ³•å®Œå…¨é‡å çš„é—®é¢˜
    
    Args:
        img1_path (str): ç¬¬ä¸€æœŸå›¾åƒè·¯å¾„
        img2_path (str): ç¬¬äºŒæœŸå›¾åƒè·¯å¾„
    """
    detector = IllegalBuildingDetector()
    
    print("=== å¼¹æ€§é…å‡†ç›¸ä¼¼æ€§åŒ¹é… ===")
    
    # åŠ è½½å›¾åƒ
    img1 = detector.load_image_with_chinese_path(img1_path)
    img2 = detector.load_image_with_chinese_path(img2_path)
    
    if img1 is None or img2 is None:
        return
    
    # å…ˆè¿›è¡ŒåŸºç¡€é€è§†æ ¡æ­£
    base_img, aligned_img, _, transform_info = detector.perspective_correction(img1, img2)
    
    print(f"åˆå§‹é€è§†æ ¡æ­£å†…ç‚¹ç‡: {transform_info.get('inlier_ratio', 0):.2%}")
    
    # è½¬æ¢ä¸ºç°åº¦å›¾
    gray1 = cv2.cvtColor(base_img, cv2.COLOR_BGR2GRAY)
    gray2 = cv2.cvtColor(aligned_img, cv2.COLOR_BGR2GRAY)
    
    # 1. åŸºäºæ»‘åŠ¨çª—å£çš„å±€éƒ¨ç›¸ä¼¼æ€§åŒ¹é…
    def local_similarity_matching(img1, img2, window_size=64, stride=32, threshold=0.7):
        """
        å±€éƒ¨ç›¸ä¼¼æ€§åŒ¹é…ï¼Œæ‰¾åˆ°å¤§é¢ç§¯ç›¸ä¼¼åŒºåŸŸ
        """
        h1, w1 = img1.shape
        h2, w2 = img2.shape
        
        matches = []
        similarity_map = np.zeros_like(img1, dtype=np.float32)
        
        print(f"  æ‰§è¡Œå±€éƒ¨ç›¸ä¼¼æ€§åŒ¹é… (çª—å£: {window_size}Ã—{window_size}, æ­¥é•¿: {stride})...")
        
        for y1 in range(0, h1 - window_size, stride):
            for x1 in range(0, w1 - window_size, stride):
                # æå–ç¬¬ä¸€æœŸå›¾åƒçš„çª—å£
                window1 = img1[y1:y1+window_size, x1:x1+window_size]
                
                # åœ¨ç¬¬äºŒæœŸå›¾åƒä¸­æœç´¢æœ€ä½³åŒ¹é…
                best_similarity = -1
                best_match = None
                
                # æœç´¢èŒƒå›´ï¼ˆå…è®¸ä¸€å®šçš„åç§»ï¼‰
                search_range = 50
                y2_start = max(0, y1 - search_range)
                y2_end = min(h2 - window_size, y1 + search_range)
                x2_start = max(0, x1 - search_range)
                x2_end = min(w2 - window_size, x1 + search_range)
                
                for y2 in range(y2_start, y2_end, stride//2):
                    for x2 in range(x2_start, x2_end, stride//2):
                        window2 = img2[y2:y2+window_size, x2:x2+window_size]
                        
                        # è®¡ç®—å½’ä¸€åŒ–äº’ç›¸å…³
                        correlation = cv2.matchTemplate(window1, window2, cv2.TM_CCOEFF_NORMED)[0, 0]
                        
                        if correlation > best_similarity:
                            best_similarity = correlation
                            best_match = (x2 + window_size//2, y2 + window_size//2)
                
                # è®°å½•é«˜è´¨é‡åŒ¹é…
                if best_similarity > threshold:
                    center1 = (x1 + window_size//2, y1 + window_size//2)
                    matches.append((center1, best_match, best_similarity))
                    
                    # æ›´æ–°ç›¸ä¼¼æ€§åœ°å›¾
                    similarity_map[y1:y1+window_size, x1:x1+window_size] = best_similarity
        
        print(f"  æ‰¾åˆ° {len(matches)} ä¸ªé«˜è´¨é‡å±€éƒ¨åŒ¹é…")
        return matches, similarity_map
    
    # æ‰§è¡Œå±€éƒ¨ç›¸ä¼¼æ€§åŒ¹é…
    local_matches, similarity_map = local_similarity_matching(gray1, gray2)
    
    # 2. åŸºäºç›¸ä¼¼åŒºåŸŸçš„ç¨ å¯†å…‰æµè®¡ç®—
    def compute_dense_flow_from_matches(img1, img2, matches):
        """
        åŸºäºåŒ¹é…ç‚¹è®¡ç®—ç¨ å¯†å…‰æµåœº
        """
        print("  è®¡ç®—ç¨ å¯†å…‰æµåœº...")
        
        if len(matches) < 10:
            print("  åŒ¹é…ç‚¹ä¸è¶³ï¼Œä½¿ç”¨å…¨å±€å…‰æµ")
            # ä½¿ç”¨Farnebackå…‰æµä½œä¸ºå¤‡é€‰
            flow = cv2.calcOpticalFlowPyrLK(img1, img2, None, None)
            return flow
        
        # åˆ›å»ºç¨€ç–å…‰æµåœº
        h, w = img1.shape
        flow_x = np.zeros((h, w), dtype=np.float32)
        flow_y = np.zeros((h, w), dtype=np.float32)
        weights = np.zeros((h, w), dtype=np.float32)
        
        # åŸºäºåŒ¹é…ç‚¹æ’å€¼å…‰æµ
        for (x1, y1), (x2, y2), similarity in matches:
            dx = x2 - x1
            dy = y2 - y1
            
            # åœ¨åŒ¹é…ç‚¹å‘¨å›´åº”ç”¨å…‰æµ
            radius = 32
            y_min = max(0, int(y1) - radius)
            y_max = min(h, int(y1) + radius)
            x_min = max(0, int(x1) - radius)
            x_max = min(w, int(x1) + radius)
            
            for y in range(y_min, y_max):
                for x in range(x_min, x_max):
                    distance = np.sqrt((x - x1)**2 + (y - y1)**2)
                    if distance < radius:
                        weight = similarity * np.exp(-distance / radius)
                        flow_x[y, x] += dx * weight
                        flow_y[y, x] += dy * weight
                        weights[y, x] += weight
        
        # å½’ä¸€åŒ–
        mask = weights > 0
        flow_x[mask] /= weights[mask]
        flow_y[mask] /= weights[mask]
        
        # å¹³æ»‘å…‰æµåœº
        flow_x = cv2.GaussianBlur(flow_x, (15, 15), 5)
        flow_y = cv2.GaussianBlur(flow_y, (15, 15), 5)
        
        return np.stack([flow_x, flow_y], axis=2)
    
    # è®¡ç®—å…‰æµåœº
    flow_field = compute_dense_flow_from_matches(gray1, gray2, local_matches)
    
    # 3. åº”ç”¨å¼¹æ€§å˜å½¢
    def apply_elastic_deformation(img, flow_field):
        """
        åº”ç”¨å¼¹æ€§å˜å½¢
        """
        print("  åº”ç”¨å¼¹æ€§å˜å½¢...")
        
        h, w = img.shape[:2]
        
        # åˆ›å»ºå˜å½¢ç½‘æ ¼
        y_coords, x_coords = np.mgrid[0:h, 0:w].astype(np.float32)
        
        if len(flow_field.shape) == 3:
            x_coords += flow_field[:, :, 0]
            y_coords += flow_field[:, :, 1]
        
        # åº”ç”¨é‡æ˜ å°„
        if len(img.shape) == 3:
            warped = cv2.remap(img, x_coords, y_coords, cv2.INTER_LINEAR)
        else:
            warped = cv2.remap(img, x_coords, y_coords, cv2.INTER_LINEAR)
        
        return warped
    
    # å¯¹ç¬¬äºŒæœŸå›¾åƒåº”ç”¨å¼¹æ€§å˜å½¢
    if isinstance(flow_field, np.ndarray) and flow_field.size > 0:
        elastically_aligned = apply_elastic_deformation(aligned_img, flow_field)
        elastically_aligned_gray = apply_elastic_deformation(gray2, flow_field)
    else:
        print("  å…‰æµè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ ¡æ­£ç»“æœ")
        elastically_aligned = aligned_img
        elastically_aligned_gray = gray2
    
    # 4. è´¨é‡è¯„ä¼°
    from skimage.metrics import structural_similarity as ssim
    
    # è®¡ç®—æ”¹è¿›åçš„ç›¸ä¼¼æ€§æŒ‡æ ‡
    ssim_before = ssim(gray1, gray2)
    ssim_after = ssim(gray1, elastically_aligned_gray)
    
    print(f"\nğŸ“Š å¼¹æ€§é…å‡†æ•ˆæœ:")
    print(f"   é€è§†æ ¡æ­£å SSIM: {ssim_before:.4f}")
    print(f"   å¼¹æ€§é…å‡†å SSIM: {ssim_after:.4f}")
    print(f"   æ”¹è¿›å¹…åº¦: {(ssim_after - ssim_before):.4f}")
    
    # 5. åˆ›å»ºå¯è§†åŒ–
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    fig, axes = plt.subplots(3, 4, figsize=(20, 15))
    fig.suptitle('å¼¹æ€§é…å‡†ç›¸ä¼¼æ€§åŒ¹é…ç»“æœ', fontsize=16, fontweight='bold')
    
    # ç¬¬ä¸€è¡Œï¼šåŸå§‹å¯¹æ¯”
    axes[0, 0].imshow(cv2.cvtColor(base_img, cv2.COLOR_BGR2RGB))
    axes[0, 0].set_title('ç¬¬ä¸€æœŸå›¾åƒï¼ˆåŸºå‡†ï¼‰', fontsize=12)
    axes[0, 0].axis('off')
    
    axes[0, 1].imshow(cv2.cvtColor(aligned_img, cv2.COLOR_BGR2RGB))
    axes[0, 1].set_title(f'é€è§†æ ¡æ­£å\nSSIM: {ssim_before:.3f}', fontsize=12)
    axes[0, 1].axis('off')
    
    axes[0, 2].imshow(cv2.cvtColor(elastically_aligned, cv2.COLOR_BGR2RGB))
    axes[0, 2].set_title(f'å¼¹æ€§é…å‡†å\nSSIM: {ssim_after:.3f}', fontsize=12)
    axes[0, 2].axis('off')
    
    # ç›¸ä¼¼æ€§åœ°å›¾
    axes[0, 3].imshow(similarity_map, cmap='viridis')
    axes[0, 3].set_title(f'ç›¸ä¼¼æ€§åœ°å›¾\nåŒ¹é…ç‚¹: {len(local_matches)}', fontsize=12)
    axes[0, 3].axis('off')
    
    # ç¬¬äºŒè¡Œï¼šé‡å æ£€æŸ¥
    # é€è§†æ ¡æ­£é‡å 
    alpha_blend_before = cv2.addWeighted(base_img, 0.5, aligned_img, 0.5, 0)
    axes[1, 0].imshow(cv2.cvtColor(alpha_blend_before, cv2.COLOR_BGR2RGB))
    axes[1, 0].set_title('é€è§†æ ¡æ­£é‡å ', fontsize=12)
    axes[1, 0].axis('off')
    
    # å¼¹æ€§é…å‡†é‡å 
    alpha_blend_after = cv2.addWeighted(base_img, 0.5, elastically_aligned, 0.5, 0)
    axes[1, 1].imshow(cv2.cvtColor(alpha_blend_after, cv2.COLOR_BGR2RGB))
    axes[1, 1].set_title('å¼¹æ€§é…å‡†é‡å ', fontsize=12)
    axes[1, 1].axis('off')
    
    # å·®å¼‚å¯¹æ¯”
    diff_before = cv2.absdiff(gray1, gray2)
    diff_after = cv2.absdiff(gray1, elastically_aligned_gray)
    
    axes[1, 2].imshow(diff_before, cmap='hot')
    axes[1, 2].set_title(f'é€è§†æ ¡æ­£å·®å¼‚\nå¹³å‡: {np.mean(diff_before):.1f}', fontsize=12)
    axes[1, 2].axis('off')
    
    axes[1, 3].imshow(diff_after, cmap='hot')
    axes[1, 3].set_title(f'å¼¹æ€§é…å‡†å·®å¼‚\nå¹³å‡: {np.mean(diff_after):.1f}', fontsize=12)
    axes[1, 3].axis('off')
    
    # ç¬¬ä¸‰è¡Œï¼šå…‰æµåœºå’Œç»Ÿè®¡
    if isinstance(flow_field, np.ndarray) and len(flow_field.shape) == 3:
        # å…‰æµåœºå¯è§†åŒ–
        flow_magnitude = np.sqrt(flow_field[:, :, 0]**2 + flow_field[:, :, 1]**2)
        axes[2, 0].imshow(flow_magnitude, cmap='jet')
        axes[2, 0].set_title('å…‰æµåœºå¼ºåº¦', fontsize=12)
        axes[2, 0].axis('off')
        
        # å…‰æµæ–¹å‘
        flow_angle = np.arctan2(flow_field[:, :, 1], flow_field[:, :, 0])
        axes[2, 1].imshow(flow_angle, cmap='hsv')
        axes[2, 1].set_title('å…‰æµæ–¹å‘', fontsize=12)
        axes[2, 1].axis('off')
    else:
        axes[2, 0].text(0.5, 0.5, 'å…‰æµè®¡ç®—å¤±è´¥', ha='center', va='center', transform=axes[2, 0].transAxes)
        axes[2, 0].axis('off')
        axes[2, 1].text(0.5, 0.5, 'æ— å…‰æµæ•°æ®', ha='center', va='center', transform=axes[2, 1].transAxes)
        axes[2, 1].axis('off')
    
    # ç»Ÿè®¡ä¿¡æ¯
    improvement_text = f"""é…å‡†æ”¹è¿›ç»Ÿè®¡:

SSIMæ”¹è¿›:
  é€è§†æ ¡æ­£: {ssim_before:.4f}
  å¼¹æ€§é…å‡†: {ssim_after:.4f}
  æ”¹è¿›å¹…åº¦: {ssim_after - ssim_before:+.4f}

å·®å¼‚å‡å°‘:
  æ ¡æ­£å‰: {np.mean(diff_before):.1f}
  æ ¡æ­£å: {np.mean(diff_after):.1f}
  å‡å°‘: {np.mean(diff_before) - np.mean(diff_after):.1f}

åŒ¹é…è´¨é‡:
  å±€éƒ¨åŒ¹é…: {len(local_matches)}
  å¹³å‡ç›¸ä¼¼æ€§: {np.mean([m[2] for m in local_matches]) if local_matches else 0:.3f}

æ€»ä½“è¯„ä¼°:
{'æ˜¾è‘—æ”¹è¿›' if ssim_after - ssim_before > 0.1 else 'é€‚åº¦æ”¹è¿›' if ssim_after - ssim_before > 0.05 else 'è½»å¾®æ”¹è¿›'}
"""
    
    color = 'lightgreen' if ssim_after - ssim_before > 0.1 else 'lightyellow' if ssim_after - ssim_before > 0.05 else 'lightcoral'
    axes[2, 2].text(0.05, 0.95, improvement_text, ha='left', va='top', transform=axes[2, 2].transAxes,
                    fontsize=10, bbox=dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.8))
    axes[2, 2].set_xlim(0, 1)
    axes[2, 2].set_ylim(0, 1)
    axes[2, 2].axis('off')
    axes[2, 2].set_title('æ”¹è¿›ç»Ÿè®¡', fontsize=12)
    
    # æŠ€æœ¯è¯´æ˜
    tech_text = """å¼¹æ€§é…å‡†æŠ€æœ¯:

1. å±€éƒ¨ç›¸ä¼¼æ€§åŒ¹é…
   â€¢ æ»‘åŠ¨çª—å£æœç´¢
   â€¢ å½’ä¸€åŒ–äº’ç›¸å…³
   â€¢ è‡ªé€‚åº”é˜ˆå€¼ç­›é€‰

2. ç¨ å¯†å…‰æµè®¡ç®—
   â€¢ åŸºäºåŒ¹é…ç‚¹æ’å€¼
   â€¢ é«˜æ–¯æƒé‡è¡°å‡
   â€¢ å…‰æµåœºå¹³æ»‘

3. å¼¹æ€§å˜å½¢åº”ç”¨
   â€¢ åŒçº¿æ€§æ’å€¼
   â€¢ ä¿æŒå±€éƒ¨è¿ç»­æ€§
   â€¢ é¿å…è¿‡åº¦å˜å½¢

ä¼˜åŠ¿:
â€¢ å¤„ç†å‡ ä½•å˜å½¢
â€¢ ä¿æŒç›¸ä¼¼åŒºåŸŸå¯¹é½
â€¢ å‡å°‘å‡é˜³æ€§å·®å¼‚
"""
    
    axes[2, 3].text(0.05, 0.95, tech_text, ha='left', va='top', transform=axes[2, 3].transAxes,
                    fontsize=9, bbox=dict(boxstyle='round,pad=0.3', facecolor='lightblue', alpha=0.8))
    axes[2, 3].set_xlim(0, 1)
    axes[2, 3].set_ylim(0, 1)
    axes[2, 3].axis('off')
    axes[2, 3].set_title('æŠ€æœ¯è¯´æ˜', fontsize=12)
    
    plt.tight_layout()
    plt.show()
    
    return {
        'original_img1': base_img,
        'original_img2': aligned_img,
        'elastically_aligned': elastically_aligned,
        'ssim_before': ssim_before,
        'ssim_after': ssim_after,
        'improvement': ssim_after - ssim_before,
        'local_matches': len(local_matches),
        'flow_field': flow_field
    }


def improved_elastic_registration_with_bbox_detection(img1_path, img2_path):
    """
    æ”¹è¿›çš„å¼¹æ€§é…å‡†æ–¹æ³•ï¼Œè§£å†³é»‘è‰²æ©ç é—®é¢˜å¹¶æä¾›bboxæ£€æµ‹
    
    Args:
        img1_path (str): ç¬¬ä¸€æœŸå›¾åƒè·¯å¾„
        img2_path (str): ç¬¬äºŒæœŸå›¾åƒè·¯å¾„
    
    Returns:
        dict: åŒ…å«é…å‡†ç»“æœå’Œbboxæ£€æµ‹ä¿¡æ¯
    """
    detector = IllegalBuildingDetector()
    
    print("=== æ”¹è¿›å¼¹æ€§é…å‡†ä¸ç›®æ ‡æ£€æµ‹ ===")
    
    # åŠ è½½å›¾åƒ
    img1 = detector.load_image_with_chinese_path(img1_path)
    img2 = detector.load_image_with_chinese_path(img2_path)
    
    if img1 is None or img2 is None:
        return None
    
    # é€è§†æ ¡æ­£
    base_img, aligned_img, _, transform_info = detector.perspective_correction(img1, img2)
    
    # 1. åˆ›å»ºæœ‰æ•ˆåŒºåŸŸæ©ç ï¼Œæ’é™¤é»‘è‰²è¾¹ç¼˜
    def create_valid_mask(img, threshold=10):
        """
        åˆ›å»ºæœ‰æ•ˆåŒºåŸŸæ©ç ï¼Œæ’é™¤é€è§†æ ¡æ­£äº§ç”Ÿçš„é»‘è‰²è¾¹ç¼˜
        
        Args:
            img: è¾“å…¥å›¾åƒ
            threshold: é»‘è‰²åŒºåŸŸé˜ˆå€¼
        
        Returns:
            mask: æœ‰æ•ˆåŒºåŸŸæ©ç 
        """
        if len(img.shape) == 3:
            # å½©è‰²å›¾åƒï¼šæ£€æŸ¥æ‰€æœ‰é€šé“
            mask = np.any(img > threshold, axis=2)
        else:
            # ç°åº¦å›¾åƒ
            mask = img > threshold
        
        # å½¢æ€å­¦æ“ä½œï¼Œæ¸…ç†æ©ç 
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))
        mask = cv2.morphologyEx(mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)
        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
        
        return mask.astype(bool)
    
    # åˆ›å»ºä¸¤å¼ å›¾åƒçš„æœ‰æ•ˆåŒºåŸŸæ©ç 
    mask1 = create_valid_mask(base_img)
    mask2 = create_valid_mask(aligned_img)
    
    # è®¡ç®—å…¬å…±æœ‰æ•ˆåŒºåŸŸ
    common_mask = np.logical_and(mask1, mask2)
    
    print(f"  æœ‰æ•ˆåŒºåŸŸæ¯”ä¾‹: {np.sum(common_mask) / common_mask.size * 100:.1f}%")
    
    # 2. åœ¨æœ‰æ•ˆåŒºåŸŸå†…è¿›è¡Œå¼¹æ€§é…å‡†
    def masked_similarity_matching(img1, img2, mask, window_size=64, stride=32, threshold=0.7):
        """
        åœ¨æ©ç åŒºåŸŸå†…è¿›è¡Œç›¸ä¼¼æ€§åŒ¹é…
        """
        h, w = img1.shape[:2]
        matches = []
        similarity_map = np.zeros_like(img1, dtype=np.float32)
        
        print(f"  åœ¨æœ‰æ•ˆåŒºåŸŸå†…æ‰§è¡Œç›¸ä¼¼æ€§åŒ¹é…...")
        
        for y1 in range(0, h - window_size, stride):
            for x1 in range(0, w - window_size, stride):
                # æ£€æŸ¥çª—å£æ˜¯å¦åœ¨æœ‰æ•ˆåŒºåŸŸå†…
                window_mask = mask[y1:y1+window_size, x1:x1+window_size]
                if np.sum(window_mask) < window_size * window_size * 0.8:  # 80%æœ‰æ•ˆåƒç´ 
                    continue
                
                window1 = img1[y1:y1+window_size, x1:x1+window_size]
                best_similarity = -1
                best_match = None
                
                search_range = 50
                y2_start = max(0, y1 - search_range)
                y2_end = min(h - window_size, y1 + search_range)
                x2_start = max(0, x1 - search_range)
                x2_end = min(w - window_size, x1 + search_range)
                
                for y2 in range(y2_start, y2_end, stride//2):
                    for x2 in range(x2_start, x2_end, stride//2):
                        # æ£€æŸ¥æœç´¢çª—å£æ˜¯å¦åœ¨æœ‰æ•ˆåŒºåŸŸå†…
                        search_mask = mask[y2:y2+window_size, x2:x2+window_size]
                        if np.sum(search_mask) < window_size * window_size * 0.8:
                            continue
                        
                        window2 = img2[y2:y2+window_size, x2:x2+window_size]
                        
                        # åªåœ¨æœ‰æ•ˆåŒºåŸŸè®¡ç®—ç›¸å…³æ€§
                        masked_window1 = window1 * window_mask[:,:,np.newaxis] if len(window1.shape)==3 else window1 * window_mask
                        masked_window2 = window2 * search_mask[:,:,np.newaxis] if len(window2.shape)==3 else window2 * search_mask
                        
                        if len(masked_window1.shape) == 3:
                            masked_window1 = cv2.cvtColor(masked_window1, cv2.COLOR_BGR2GRAY)
                            masked_window2 = cv2.cvtColor(masked_window2, cv2.COLOR_BGR2GRAY)
                        
                        correlation = cv2.matchTemplate(masked_window1, masked_window2, cv2.TM_CCOEFF_NORMED)[0, 0]
                        
                        if correlation > best_similarity:
                            best_similarity = correlation
                            best_match = (x2 + window_size//2, y2 + window_size//2)
                
                if best_similarity > threshold:
                    center1 = (x1 + window_size//2, y1 + window_size//2)
                    matches.append((center1, best_match, best_similarity))
                    similarity_map[y1:y1+window_size, x1:x1+window_size] = best_similarity
        
        print(f"  æ‰¾åˆ° {len(matches)} ä¸ªæœ‰æ•ˆåŒºåŸŸå†…çš„åŒ¹é…")
        return matches, similarity_map
    
    # è½¬æ¢ä¸ºç°åº¦å›¾
    gray1 = cv2.cvtColor(base_img, cv2.COLOR_BGR2GRAY)
    gray2 = cv2.cvtColor(aligned_img, cv2.COLOR_BGR2GRAY)
    
    # æ‰§è¡Œæ©ç å†…çš„ç›¸ä¼¼æ€§åŒ¹é…
    local_matches, similarity_map = masked_similarity_matching(gray1, gray2, common_mask)
    
    # 3. è®¡ç®—å…‰æµå¹¶åº”ç”¨å¼¹æ€§å˜å½¢
    def compute_masked_dense_flow(img1, img2, matches, mask):
        """
        åœ¨æ©ç åŒºåŸŸå†…è®¡ç®—ç¨ å¯†å…‰æµ
        """
        if len(matches) < 5:
            print("  åŒ¹é…ç‚¹ä¸è¶³ï¼Œè·³è¿‡å¼¹æ€§å˜å½¢")
            return None
        
        h, w = img1.shape
        flow_x = np.zeros((h, w), dtype=np.float32)
        flow_y = np.zeros((h, w), dtype=np.float32)
        weights = np.zeros((h, w), dtype=np.float32)
        
        for (x1, y1), (x2, y2), similarity in matches:
            dx = x2 - x1
            dy = y2 - y1
            
            radius = 40
            y_min = max(0, int(y1) - radius)
            y_max = min(h, int(y1) + radius)
            x_min = max(0, int(x1) - radius)
            x_max = min(w, int(x1) + radius)
            
            for y in range(y_min, y_max):
                for x in range(x_min, x_max):
                    if mask[y, x]:  # åªåœ¨æœ‰æ•ˆåŒºåŸŸå†…åº”ç”¨
                        distance = np.sqrt((x - x1)**2 + (y - y1)**2)
                        if distance < radius:
                            weight = similarity * np.exp(-distance / radius)
                            flow_x[y, x] += dx * weight
                            flow_y[y, x] += dy * weight
                            weights[y, x] += weight
        
        # å½’ä¸€åŒ–
        valid_flow = weights > 0
        flow_x[valid_flow] /= weights[valid_flow]
        flow_y[valid_flow] /= weights[valid_flow]
        
        # åªåœ¨æœ‰æ•ˆåŒºåŸŸå†…å¹³æ»‘
        flow_x = cv2.GaussianBlur(flow_x, (15, 15), 5)
        flow_y = cv2.GaussianBlur(flow_y, (15, 15), 5)
        
        return np.stack([flow_x, flow_y], axis=2)
    
    # è®¡ç®—å…‰æµåœº
    flow_field = compute_masked_dense_flow(gray1, gray2, local_matches, common_mask)
    
    # åº”ç”¨å¼¹æ€§å˜å½¢
    if flow_field is not None:
        h, w = aligned_img.shape[:2]
        y_coords, x_coords = np.mgrid[0:h, 0:w].astype(np.float32)
        x_coords += flow_field[:, :, 0]
        y_coords += flow_field[:, :, 1]
        
        elastically_aligned = cv2.remap(aligned_img, x_coords, y_coords, cv2.INTER_LINEAR)
        elastically_aligned_gray = cv2.remap(gray2, x_coords, y_coords, cv2.INTER_LINEAR)
    else:
        elastically_aligned = aligned_img
        elastically_aligned_gray = gray2
    
    # 4. åœ¨æœ‰æ•ˆåŒºåŸŸå†…è®¡ç®—å·®å¼‚
    def compute_masked_difference(img1, img2, mask):
        """
        åœ¨æ©ç åŒºåŸŸå†…è®¡ç®—å·®å¼‚
        """
        diff = cv2.absdiff(img1, img2)
        # å°†æ— æ•ˆåŒºåŸŸè®¾ä¸º0
        diff[~mask] = 0
        return diff
    
    # è®¡ç®—æ©ç å·®å¼‚
    masked_diff = compute_masked_difference(gray1, elastically_aligned_gray, common_mask)
    
    # 5. å·®å¼‚åŒºåŸŸæ£€æµ‹å’Œbboxæå–
    def detect_difference_bboxes(diff_img, mask, min_area=500, threshold=50):
        """
        æ£€æµ‹å·®å¼‚åŒºåŸŸå¹¶è¿”å›bboxä¿¡æ¯
        
        Args:
            diff_img: å·®å¼‚å›¾åƒ
            mask: æœ‰æ•ˆåŒºåŸŸæ©ç 
            min_area: æœ€å°åŒºåŸŸé¢ç§¯
            threshold: å·®å¼‚é˜ˆå€¼
        
        Returns:
            list: bboxä¿¡æ¯åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« [x, y, w, h, confidence, area]
        """
        print(f"  æ£€æµ‹å·®å¼‚åŒºåŸŸ...")
        
        # äºŒå€¼åŒ–å·®å¼‚å›¾åƒ
        binary_diff = (diff_img > threshold).astype(np.uint8)
        binary_diff = binary_diff * mask.astype(np.uint8)  # åªä¿ç•™æœ‰æ•ˆåŒºåŸŸ
        
        # å½¢æ€å­¦æ“ä½œï¼Œè¿æ¥ç›¸è¿‘çš„å·®å¼‚åŒºåŸŸ
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))
        binary_diff = cv2.morphologyEx(binary_diff, cv2.MORPH_CLOSE, kernel)
        binary_diff = cv2.morphologyEx(binary_diff, cv2.MORPH_OPEN, 
                                      cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))
        
        # æŸ¥æ‰¾è½®å»“
        contours, _ = cv2.findContours(binary_diff, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        bboxes = []
        for i, contour in enumerate(contours):
            area = cv2.contourArea(contour)
            if area < min_area:
                continue
            
            # è®¡ç®—è¾¹ç•Œæ¡†
            x, y, w, h = cv2.boundingRect(contour)
            
            # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºåŒºåŸŸå†…çš„å¹³å‡å·®å¼‚ï¼‰
            roi_diff = diff_img[y:y+h, x:x+w]
            roi_mask = mask[y:y+h, x:x+w]
            
            if np.sum(roi_mask) == 0:
                continue
                
            avg_diff = np.mean(roi_diff[roi_mask])
            max_diff = np.max(roi_diff[roi_mask])
            
            # ç½®ä¿¡åº¦è®¡ç®—ï¼šç»“åˆå¹³å‡å·®å¼‚å’Œæœ€å¤§å·®å¼‚
            confidence = min(1.0, (avg_diff / 255.0) * 0.7 + (max_diff / 255.0) * 0.3)
            
            bboxes.append({
                'id': i + 1,
                'bbox': [int(x), int(y), int(w), int(h)],
                'confidence': float(confidence),
                'area': float(area),
                'avg_difference': float(avg_diff),
                'max_difference': float(max_diff),
                'center': [int(x + w/2), int(y + h/2)]
            })
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        bboxes.sort(key=lambda x: x['confidence'], reverse=True)
        
        print(f"  æ£€æµ‹åˆ° {len(bboxes)} ä¸ªå·®å¼‚åŒºåŸŸ")
        return bboxes
    
    # æ£€æµ‹å·®å¼‚åŒºåŸŸ
    detected_bboxes = detect_difference_bboxes(masked_diff, common_mask)
    
    # 6. è´¨é‡è¯„ä¼°
    from skimage.metrics import structural_similarity as ssim
    
    # åªåœ¨æœ‰æ•ˆåŒºåŸŸè®¡ç®—SSIM
    ssim_before = ssim(gray1 * common_mask, gray2 * common_mask)
    ssim_after = ssim(gray1 * common_mask, elastically_aligned_gray * common_mask)
    
    print(f"\nğŸ“Š æ”¹è¿›é…å‡†æ•ˆæœ:")
    print(f"   æœ‰æ•ˆåŒºåŸŸSSIM: {ssim_before:.4f} â†’ {ssim_after:.4f}")
    print(f"   æ”¹è¿›å¹…åº¦: {ssim_after - ssim_before:+.4f}")
    print(f"   æ£€æµ‹åŒºåŸŸæ•°: {len(detected_bboxes)}")
    
    # 7. åˆ›å»ºå¯è§†åŒ–
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    fig, axes = plt.subplots(3, 4, figsize=(20, 15))
    fig.suptitle('æ”¹è¿›å¼¹æ€§é…å‡†ä¸ç›®æ ‡æ£€æµ‹ç»“æœ', fontsize=16, fontweight='bold')
    
    # ç¬¬ä¸€è¡Œï¼šæ©ç å’Œé…å‡†å¯¹æ¯”
    axes[0, 0].imshow(cv2.cvtColor(base_img, cv2.COLOR_BGR2RGB))
    axes[0, 0].set_title('ç¬¬ä¸€æœŸå›¾åƒï¼ˆåŸºå‡†ï¼‰', fontsize=12)
    axes[0, 0].axis('off')
    
    axes[0, 1].imshow(cv2.cvtColor(aligned_img, cv2.COLOR_BGR2RGB))
    axes[0, 1].set_title('é€è§†æ ¡æ­£åï¼ˆæœ‰é»‘è¾¹ï¼‰', fontsize=12)
    axes[0, 1].axis('off')
    
    # æ˜¾ç¤ºæœ‰æ•ˆåŒºåŸŸæ©ç 
    axes[0, 2].imshow(common_mask, cmap='gray')
    axes[0, 2].set_title(f'æœ‰æ•ˆåŒºåŸŸæ©ç \nè¦†ç›–ç‡: {np.sum(common_mask)/common_mask.size*100:.1f}%', fontsize=12)
    axes[0, 2].axis('off')
    
    axes[0, 3].imshow(cv2.cvtColor(elastically_aligned, cv2.COLOR_BGR2RGB))
    axes[0, 3].set_title(f'å¼¹æ€§é…å‡†å\nSSIM: {ssim_after:.3f}', fontsize=12)
    axes[0, 3].axis('off')
    
    # ç¬¬äºŒè¡Œï¼šå·®å¼‚æ£€æµ‹
    axes[1, 0].imshow(masked_diff, cmap='hot')
    axes[1, 0].set_title(f'æ©ç å·®å¼‚å›¾\nå¹³å‡å·®å¼‚: {np.mean(masked_diff[common_mask]):.1f}', fontsize=12)
    axes[1, 0].axis('off')
    
    # ç»˜åˆ¶æ£€æµ‹ç»“æœ
    detection_img = elastically_aligned.copy()
    for bbox_info in detected_bboxes:
        x, y, w, h = bbox_info['bbox']
        confidence = bbox_info['confidence']
        
        # æ ¹æ®ç½®ä¿¡åº¦é€‰æ‹©é¢œè‰²
        if confidence > 0.8:
            color = (0, 0, 255)  # çº¢è‰²ï¼šé«˜ç½®ä¿¡åº¦
        elif confidence > 0.6:
            color = (0, 165, 255)  # æ©™è‰²ï¼šä¸­ç­‰ç½®ä¿¡åº¦
        else:
            color = (0, 255, 255)  # é»„è‰²ï¼šä½ç½®ä¿¡åº¦
        
        cv2.rectangle(detection_img, (x, y), (x+w, y+h), color, 2)
        cv2.putText(detection_img, f"ID:{bbox_info['id']} ({confidence:.2f})", 
                   (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
    
    axes[1, 1].imshow(cv2.cvtColor(detection_img, cv2.COLOR_BGR2RGB))
    axes[1, 1].set_title(f'ç›®æ ‡æ£€æµ‹ç»“æœ\næ£€æµ‹æ•°: {len(detected_bboxes)}', fontsize=12)
    axes[1, 1].axis('off')
    
    # ç½®ä¿¡åº¦åˆ†å¸ƒ
    if detected_bboxes:
        confidences = [b['confidence'] for b in detected_bboxes]
        axes[1, 2].hist(confidences, bins=10, alpha=0.7, color='blue')
        axes[1, 2].set_title('ç½®ä¿¡åº¦åˆ†å¸ƒ', fontsize=12)
        axes[1, 2].set_xlabel('ç½®ä¿¡åº¦')
        axes[1, 2].set_ylabel('æ•°é‡')
    else:
        axes[1, 2].text(0.5, 0.5, 'æ— æ£€æµ‹ç»“æœ', ha='center', va='center', 
                       transform=axes[1, 2].transAxes, fontsize=14)
        axes[1, 2].set_title('ç½®ä¿¡åº¦åˆ†å¸ƒ', fontsize=12)
    
    # é¢ç§¯åˆ†å¸ƒ
    if detected_bboxes:
        areas = [b['area'] for b in detected_bboxes]
        axes[1, 3].hist(areas, bins=10, alpha=0.7, color='green')
        axes[1, 3].set_title('åŒºåŸŸé¢ç§¯åˆ†å¸ƒ', fontsize=12)
        axes[1, 3].set_xlabel('é¢ç§¯ (åƒç´ )')
        axes[1, 3].set_ylabel('æ•°é‡')
    else:
        axes[1, 3].text(0.5, 0.5, 'æ— æ£€æµ‹ç»“æœ', ha='center', va='center', 
                       transform=axes[1, 3].transAxes, fontsize=14)
        axes[1, 3].set_title('åŒºåŸŸé¢ç§¯åˆ†å¸ƒ', fontsize=12)
    
    # ç¬¬ä¸‰è¡Œï¼šè¯¦ç»†ä¿¡æ¯
    # æ£€æµ‹ç»Ÿè®¡
    if detected_bboxes:
        high_conf = len([b for b in detected_bboxes if b['confidence'] > 0.8])
        med_conf = len([b for b in detected_bboxes if 0.6 < b['confidence'] <= 0.8])
        low_conf = len([b for b in detected_bboxes if b['confidence'] <= 0.6])
        
        stats_text = f"""æ£€æµ‹ç»Ÿè®¡ä¿¡æ¯:

æ€»æ£€æµ‹æ•°: {len(detected_bboxes)}
é«˜ç½®ä¿¡åº¦ (>0.8): {high_conf}
ä¸­ç­‰ç½®ä¿¡åº¦ (0.6-0.8): {med_conf}
ä½ç½®ä¿¡åº¦ (â‰¤0.6): {low_conf}

å¹³å‡ç½®ä¿¡åº¦: {np.mean([b['confidence'] for b in detected_bboxes]):.3f}
å¹³å‡é¢ç§¯: {np.mean([b['area'] for b in detected_bboxes]):.0f} pxÂ²

æœ€å¤§å·®å¼‚åŒºåŸŸ:
ID: {detected_bboxes[0]['id']}
ç½®ä¿¡åº¦: {detected_bboxes[0]['confidence']:.3f}
é¢ç§¯: {detected_bboxes[0]['area']:.0f} pxÂ²
"""
    else:
        stats_text = """æ£€æµ‹ç»Ÿè®¡ä¿¡æ¯:

æœªæ£€æµ‹åˆ°æ˜¾è‘—å·®å¼‚åŒºåŸŸ

å¯èƒ½åŸå› :
â€¢ å›¾åƒå˜åŒ–å¾ˆå°
â€¢ å·®å¼‚é˜ˆå€¼è¿‡é«˜
â€¢ æœ€å°åŒºåŸŸé¢ç§¯è¿‡å¤§
â€¢ é…å‡†è´¨é‡å¾ˆå¥½
"""
    
    color = 'lightcoral' if len(detected_bboxes) > 3 else 'lightyellow' if len(detected_bboxes) > 0 else 'lightgreen'
    axes[2, 0].text(0.05, 0.95, stats_text, ha='left', va='top', transform=axes[2, 0].transAxes,
                    fontsize=10, bbox=dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.8))
    axes[2, 0].set_xlim(0, 1)
    axes[2, 0].set_ylim(0, 1)
    axes[2, 0].axis('off')
    axes[2, 0].set_title('æ£€æµ‹ç»Ÿè®¡', fontsize=12)
    
    # æŠ€æœ¯æ”¹è¿›è¯´æ˜
    tech_text = """æŠ€æœ¯æ”¹è¿›è¦ç‚¹:

1. é»‘è‰²æ©ç å¤„ç†:
   â€¢ è‡ªåŠ¨æ£€æµ‹æœ‰æ•ˆåŒºåŸŸ
   â€¢ æ’é™¤é€è§†æ ¡æ­£é»‘è¾¹
   â€¢ åªåœ¨æœ‰æ•ˆåŒºåŸŸè®¡ç®—å·®å¼‚

2. å¼¹æ€§é…å‡†ä¼˜åŒ–:
   â€¢ æ©ç å†…ç›¸ä¼¼æ€§åŒ¹é…
   â€¢ æœ‰æ•ˆåŒºåŸŸå…‰æµè®¡ç®—
   â€¢ é¿å…é»‘è¾¹å¹²æ‰°

3. ç›®æ ‡æ£€æµ‹åŠŸèƒ½:
   â€¢ å·®å¼‚åŒºåŸŸåˆ†å‰²
   â€¢ bboxåæ ‡æå–
   â€¢ ç½®ä¿¡åº¦è®¡ç®—

4. ç»“æœå¯é æ€§:
   â€¢ å½¢æ€å­¦åå¤„ç†
   â€¢ é¢ç§¯é˜ˆå€¼è¿‡æ»¤
   â€¢ å¤šæŒ‡æ ‡è¯„ä¼°
"""
    
    axes[2, 1].text(0.05, 0.95, tech_text, ha='left', va='top', transform=axes[2, 1].transAxes,
                    fontsize=9, bbox=dict(boxstyle='round,pad=0.3', facecolor='lightblue', alpha=0.8))
    axes[2, 1].set_xlim(0, 1)
    axes[2, 1].set_ylim(0, 1)
    axes[2, 1].axis('off')
    axes[2, 1].set_title('æŠ€æœ¯æ”¹è¿›', fontsize=12)
    
    # bboxè¯¦ç»†ä¿¡æ¯
    if detected_bboxes:
        bbox_text = "æ£€æµ‹åˆ°çš„åŒºåŸŸè¯¦æƒ…:\n\n"
        for i, bbox in enumerate(detected_bboxes[:5]):  # æ˜¾ç¤ºå‰5ä¸ª
            bbox_text += f"åŒºåŸŸ {bbox['id']}:\n"
            bbox_text += f"  ä½ç½®: ({bbox['bbox'][0]}, {bbox['bbox'][1]})\n"
            bbox_text += f"  å°ºå¯¸: {bbox['bbox'][2]}Ã—{bbox['bbox'][3]}\n"
            bbox_text += f"  ç½®ä¿¡åº¦: {bbox['confidence']:.3f}\n"
            bbox_text += f"  é¢ç§¯: {bbox['area']:.0f} pxÂ²\n\n"
        
        if len(detected_bboxes) > 5:
            bbox_text += f"... è¿˜æœ‰ {len(detected_bboxes)-5} ä¸ªåŒºåŸŸ"
    else:
        bbox_text = "æœªæ£€æµ‹åˆ°å·®å¼‚åŒºåŸŸ\n\nè¿™å¯èƒ½æ„å‘³ç€:\nâ€¢ å›¾åƒé…å‡†è´¨é‡å¾ˆå¥½\nâ€¢ ä¸¤æœŸå›¾åƒå˜åŒ–å¾ˆå°\nâ€¢ éœ€è¦è°ƒæ•´æ£€æµ‹å‚æ•°"
    
    axes[2, 2].text(0.05, 0.95, bbox_text, ha='left', va='top', transform=axes[2, 2].transAxes,
                    fontsize=9, bbox=dict(boxstyle='round,pad=0.3', facecolor='lightgray', alpha=0.8))
    axes[2, 2].set_xlim(0, 1)
    axes[2, 2].set_ylim(0, 1)
    axes[2, 2].axis('off')
    axes[2, 2].set_title('åŒºåŸŸè¯¦æƒ…', fontsize=12)
    
    # å‚æ•°è®¾ç½®
    params_text = f"""æ£€æµ‹å‚æ•°è®¾ç½®:

å·®å¼‚é˜ˆå€¼: 50
æœ€å°åŒºåŸŸé¢ç§¯: 500 pxÂ²
å½¢æ€å­¦æ ¸: 7Ã—7 æ¤­åœ†
æœ‰æ•ˆåŒºåŸŸé˜ˆå€¼: 10

é…å‡†å‚æ•°:
çª—å£å¤§å°: 64Ã—64
æ­¥é•¿: 32
ç›¸ä¼¼åº¦é˜ˆå€¼: 0.7
æœç´¢èŒƒå›´: Â±50 px

è´¨é‡æŒ‡æ ‡:
æœ‰æ•ˆåŒºåŸŸæ¯”ä¾‹: {np.sum(common_mask)/common_mask.size*100:.1f}%
SSIMæ”¹è¿›: {ssim_after-ssim_before:+.4f}
åŒ¹é…ç‚¹æ•°: {len(local_matches)}
"""
    
    axes[2, 3].text(0.05, 0.95, params_text, ha='left', va='top', transform=axes[2, 3].transAxes,
                    fontsize=9, bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow', alpha=0.8))
    axes[2, 3].set_xlim(0, 1)
    axes[2, 3].set_ylim(0, 1)
    axes[2, 3].axis('off')
    axes[2, 3].set_title('å‚æ•°è®¾ç½®', fontsize=12)
    
    plt.tight_layout()
    plt.show()
    
    return {
        'original_img1': base_img,
        'original_img2': aligned_img,
        'elastically_aligned': elastically_aligned,
        'valid_mask': common_mask,
        'masked_difference': masked_diff,
        'detected_bboxes': detected_bboxes,
        'ssim_before': ssim_before,
        'ssim_after': ssim_after,
        'improvement': ssim_after - ssim_before,
        'valid_area_ratio': np.sum(common_mask) / common_mask.size,
        'local_matches': len(local_matches),
        'detection_count': len(detected_bboxes)
    }